diff --git a/algo/common/include/xa_nn_fully_connected_common.h b/algo/common/include/xa_nn_fully_connected_common.h
index 84c43eb..61553bf 100644
--- a/algo/common/include/xa_nn_fully_connected_common.h
+++ b/algo/common/include/xa_nn_fully_connected_common.h
@@ -456,7 +456,6 @@ WORD32 xa_nn_fully_connected_asym4sxasym8s_asym8s
   XA_NNLIB_ARG_CHK_COND((weight_zero_bias < -127 || weight_zero_bias > 128), -1);
   XA_NNLIB_ARG_CHK_COND((out_shift < -31 || out_shift > 31), -1);
   XA_NNLIB_ARG_CHK_COND((out_zero_bias < -128 || out_zero_bias > 127), -1);
-  XA_NNLIB_ARG_CHK_COND(((weight_depth % 2) != 0), -1);
 
   WORD32 ret = 0;
   ret = xa_nn_matXvec_asym4sxasym8s_asym8s
diff --git a/algo/common/include/xa_nnlib_common_macros_hifi5.h b/algo/common/include/xa_nnlib_common_macros_hifi5.h
index 8424cb9..1550df9 100644
--- a/algo/common/include/xa_nnlib_common_macros_hifi5.h
+++ b/algo/common/include/xa_nnlib_common_macros_hifi5.h
@@ -1343,8 +1343,8 @@ ae_int16 _ae_int16_bias = ZERO16; \
           AE_MULA8Q8X8(_ae_int32x2_acc_1, _ae_int32x2_acc_0,zero_temp,zero_temp,zero_temp,_ae_int8x8_mat1_1_0,_ae_int8x8_vec1_1);
 
 #define KERNEL_8x8_NOT_UNROLLED_MAT2_VEC2 \
-          AE_MULA8Q8X8(_ae_int32x2_acc_0, _ae_int32x2_acc_0,zero_temp,zero_temp,zero_temp,_ae_int8x8_mat2_0,_ae_int8x8_vec2);\
-          AE_MULA8Q8X8(_ae_int32x2_acc_0, _ae_int32x2_acc_0,zero_temp,zero_temp,zero_temp,_ae_int8x8_mat2_1_0,_ae_int8x8_vec2_1);
+          AE_MULA8Q8X8(_ae_int32x2_acc_0, _ae_int32x2_acc_1,zero_temp,zero_temp,zero_temp,_ae_int8x8_mat2_0,_ae_int8x8_vec2);\
+          AE_MULA8Q8X8(_ae_int32x2_acc_0, _ae_int32x2_acc_1,zero_temp,zero_temp,zero_temp,_ae_int8x8_mat2_1_0,_ae_int8x8_vec2_1);
 
 #define STORE_ACC_8bx8b_AT_OUT_8_SINGLE \
         ae_int8x8 temp;\
diff --git a/algo/kernels/cnn/hifi5/xa_nn_matXvec_sym4sxasym8s_asym8s_circ.c b/algo/kernels/cnn/hifi5/xa_nn_matXvec_sym4sxasym8s_asym8s_circ.c
index f580674..88a113a 100644
--- a/algo/kernels/cnn/hifi5/xa_nn_matXvec_sym4sxasym8s_asym8s_circ.c
+++ b/algo/kernels/cnn/hifi5/xa_nn_matXvec_sym4sxasym8s_asym8s_circ.c
@@ -1181,6 +1181,7 @@ WORD32 xa_nn_matXvec_sym4sxasym8s_asym8s_circ(
       ae_int32x2 acc3 = AE_ZERO32();  
 
       ae_int32x2 dummy_acc;
+      ae_int32x2 dummy_acc1;
       ae_int8x8 mat0_0, mat0_1, mat0_2, mat0_3;
       ae_int8x8 mat1_0, mat1_1, mat1_2, mat1_3;
       ae_int8x8 vec0_0, vec0_1;
@@ -1242,15 +1243,15 @@ WORD32 xa_nn_matXvec_sym4sxasym8s_asym8s_circ(
         AE_SUBW8(mat_plus_zb1_4, mat_plus_zb1_5, mat1_2, mat_zb);
         AE_SUBW8(mat_plus_zb1_6, mat_plus_zb1_7, mat1_3, mat_zb);
 
-        AE_MULA4O4X16(acc0, dummy_acc, acc1, dummy_acc, vec_4b_0_0, vec_4b_0_0, mat_plus_zb0_0, mat_plus_zb0_1);
-        AE_MULA4O4X16(dummy_acc, acc0, dummy_acc, acc1, vec_4b_0_0, vec_4b_0_0, mat_plus_zb0_2, mat_plus_zb0_3);
-        AE_MULA4O4X16(acc0, dummy_acc, acc1, dummy_acc, vec_4b_0_1, vec_4b_0_1, mat_plus_zb0_4, mat_plus_zb0_5);
-        AE_MULA4O4X16(dummy_acc, acc0, dummy_acc, acc1, vec_4b_0_1, vec_4b_0_1, mat_plus_zb0_6, mat_plus_zb0_7);
+        AE_MULA4O4X16(acc0, dummy_acc, acc1, dummy_acc1, vec_4b_0_0, vec_4b_0_0, mat_plus_zb0_0, mat_plus_zb0_1);
+        AE_MULA4O4X16(dummy_acc, acc0, dummy_acc1, acc1, vec_4b_0_0, vec_4b_0_0, mat_plus_zb0_2, mat_plus_zb0_3);
+        AE_MULA4O4X16(acc0, dummy_acc, acc1, dummy_acc1, vec_4b_0_1, vec_4b_0_1, mat_plus_zb0_4, mat_plus_zb0_5);
+        AE_MULA4O4X16(dummy_acc, acc0, dummy_acc1, acc1, vec_4b_0_1, vec_4b_0_1, mat_plus_zb0_6, mat_plus_zb0_7);
 
-        AE_MULA4O4X16(acc2, dummy_acc, acc3, dummy_acc, vec_4b_0_0, vec_4b_0_0, mat_plus_zb1_0, mat_plus_zb1_1);
-        AE_MULA4O4X16(dummy_acc, acc2, dummy_acc, acc3, vec_4b_0_0, vec_4b_0_0, mat_plus_zb1_2, mat_plus_zb1_3);
-        AE_MULA4O4X16(acc2, dummy_acc, acc3, dummy_acc, vec_4b_0_1, vec_4b_0_1, mat_plus_zb1_4, mat_plus_zb1_5);
-        AE_MULA4O4X16(dummy_acc, acc2, dummy_acc, acc3, vec_4b_0_1, vec_4b_0_1, mat_plus_zb1_6, mat_plus_zb1_7);        
+        AE_MULA4O4X16(acc2, dummy_acc, acc3, dummy_acc1, vec_4b_0_0, vec_4b_0_0, mat_plus_zb1_0, mat_plus_zb1_1);
+        AE_MULA4O4X16(dummy_acc, acc2, dummy_acc1, acc3, vec_4b_0_0, vec_4b_0_0, mat_plus_zb1_2, mat_plus_zb1_3);
+        AE_MULA4O4X16(acc2, dummy_acc, acc3, dummy_acc1, vec_4b_0_1, vec_4b_0_1, mat_plus_zb1_4, mat_plus_zb1_5);
+        AE_MULA4O4X16(dummy_acc, acc2, dummy_acc1, acc3, vec_4b_0_1, vec_4b_0_1, mat_plus_zb1_6, mat_plus_zb1_7);
       }        
       if(cols_count != cols1)
       {
@@ -1275,15 +1276,15 @@ WORD32 xa_nn_matXvec_sym4sxasym8s_asym8s_circ(
         AE_SUBW8(mat_plus_zb1_4, mat_plus_zb1_5, mat1_2, mat_zb);
         AE_SUBW8(mat_plus_zb1_6, mat_plus_zb1_7, mat1_3, mat_zb);
 
-        AE_MULA4O4X16(acc0, dummy_acc, acc1, dummy_acc, vec_4b_0_0, vec_4b_0_0, mat_plus_zb0_0, mat_plus_zb0_1);
-        AE_MULA4O4X16(dummy_acc, acc0, dummy_acc, acc1, vec_4b_0_0, vec_4b_0_0, mat_plus_zb0_2, mat_plus_zb0_3);
-        AE_MULA4O4X16(acc0, dummy_acc, acc1, dummy_acc, vec_4b_0_1, vec_4b_0_1, mat_plus_zb0_4, mat_plus_zb0_5);
-        AE_MULA4O4X16(dummy_acc, acc0, dummy_acc, acc1, vec_4b_0_1, vec_4b_0_1, mat_plus_zb0_6, mat_plus_zb0_7);
+        AE_MULA4O4X16(acc0, dummy_acc, acc1, dummy_acc1, vec_4b_0_0, vec_4b_0_0, mat_plus_zb0_0, mat_plus_zb0_1);
+        AE_MULA4O4X16(dummy_acc, acc0, dummy_acc1, acc1, vec_4b_0_0, vec_4b_0_0, mat_plus_zb0_2, mat_plus_zb0_3);
+        AE_MULA4O4X16(acc0, dummy_acc, acc1, dummy_acc1, vec_4b_0_1, vec_4b_0_1, mat_plus_zb0_4, mat_plus_zb0_5);
+        AE_MULA4O4X16(dummy_acc, acc0, dummy_acc1, acc1, vec_4b_0_1, vec_4b_0_1, mat_plus_zb0_6, mat_plus_zb0_7);
 
-        AE_MULA4O4X16(acc2, dummy_acc, acc3, dummy_acc, vec_4b_0_0, vec_4b_0_0, mat_plus_zb1_0, mat_plus_zb1_1);
-        AE_MULA4O4X16(dummy_acc, acc2, dummy_acc, acc3, vec_4b_0_0, vec_4b_0_0, mat_plus_zb1_2, mat_plus_zb1_3);
-        AE_MULA4O4X16(acc2, dummy_acc, acc3, dummy_acc, vec_4b_0_1, vec_4b_0_1, mat_plus_zb1_4, mat_plus_zb1_5);
-        AE_MULA4O4X16(dummy_acc, acc2, dummy_acc, acc3, vec_4b_0_1, vec_4b_0_1, mat_plus_zb1_6, mat_plus_zb1_7);  
+        AE_MULA4O4X16(acc2, dummy_acc, acc3, dummy_acc1, vec_4b_0_0, vec_4b_0_0, mat_plus_zb1_0, mat_plus_zb1_1);
+        AE_MULA4O4X16(dummy_acc, acc2, dummy_acc1, acc3, vec_4b_0_0, vec_4b_0_0, mat_plus_zb1_2, mat_plus_zb1_3);
+        AE_MULA4O4X16(acc2, dummy_acc, acc3, dummy_acc1, vec_4b_0_1, vec_4b_0_1, mat_plus_zb1_4, mat_plus_zb1_5);
+        AE_MULA4O4X16(dummy_acc, acc2, dummy_acc1, acc3, vec_4b_0_1, vec_4b_0_1, mat_plus_zb1_6, mat_plus_zb1_7);
       }
       
       acc0 = AE_ADD32S_HL_LH(acc0, acc1);
@@ -1318,6 +1319,7 @@ WORD32 xa_nn_matXvec_sym4sxasym8s_asym8s_circ(
       ae_int32x2 acc0 = AE_ZERO32();
       ae_int32x2 acc1 = AE_ZERO32();     
       ae_int32x2 dummy_acc = AE_ZERO32();
+      ae_int32x2 dummy_acc1 = AE_ZERO32();
       ae_int8x8 mat0, mat1, mat2, mat3;
       ae_int8x8 vec0, vec1;
       ae_int16x4 mat_plus_zb0, mat_plus_zb1;
@@ -1354,10 +1356,10 @@ WORD32 xa_nn_matXvec_sym4sxasym8s_asym8s_circ(
         AE_SUBW8(mat_plus_zb2, mat_plus_zb3, mat1, mat_zb);
         AE_SUBW8(mat_plus_zb4, mat_plus_zb5, mat2, mat_zb);
         AE_SUBW8(mat_plus_zb6, mat_plus_zb7, mat3, mat_zb);
-        AE_MULA4O4X16(acc0, dummy_acc, acc1, dummy_acc, vec_4b_0, vec_4b_0, mat_plus_zb0, mat_plus_zb1);
-        AE_MULA4O4X16(dummy_acc, acc0, dummy_acc, acc1, vec_4b_0, vec_4b_0, mat_plus_zb2, mat_plus_zb3);
-        AE_MULA4O4X16(acc0, dummy_acc, acc1, dummy_acc, vec_4b_1, vec_4b_1, mat_plus_zb4, mat_plus_zb5);
-        AE_MULA4O4X16(dummy_acc, acc0, dummy_acc, acc1, vec_4b_1, vec_4b_1, mat_plus_zb6, mat_plus_zb7);          
+        AE_MULA4O4X16(acc0, dummy_acc, acc1, dummy_acc1, vec_4b_0, vec_4b_0, mat_plus_zb0, mat_plus_zb1);
+        AE_MULA4O4X16(dummy_acc, acc0, dummy_acc1, acc1, vec_4b_0, vec_4b_0, mat_plus_zb2, mat_plus_zb3);
+        AE_MULA4O4X16(acc0, dummy_acc, acc1, dummy_acc1, vec_4b_1, vec_4b_1, mat_plus_zb4, mat_plus_zb5);
+        AE_MULA4O4X16(dummy_acc, acc0, dummy_acc1, acc1, vec_4b_1, vec_4b_1, mat_plus_zb6, mat_plus_zb7);
       }
       if(cols_count != cols1)
       {
@@ -1372,10 +1374,10 @@ WORD32 xa_nn_matXvec_sym4sxasym8s_asym8s_circ(
         AE_SUBW8(mat_plus_zb2, mat_plus_zb3, mat1, mat_zb);
         AE_SUBW8(mat_plus_zb4, mat_plus_zb5, mat2, mat_zb);
         AE_SUBW8(mat_plus_zb6, mat_plus_zb7, mat3, mat_zb);
-        AE_MULA4O4X16(acc0, dummy_acc, acc1, dummy_acc, vec_4b_0, vec_4b_0, mat_plus_zb0, mat_plus_zb1);
-        AE_MULA4O4X16(dummy_acc, acc0, dummy_acc, acc1, vec_4b_0, vec_4b_0, mat_plus_zb2, mat_plus_zb3);
-        AE_MULA4O4X16(acc0, dummy_acc, acc1, dummy_acc, vec_4b_1, vec_4b_1, mat_plus_zb4, mat_plus_zb5);
-        AE_MULA4O4X16(dummy_acc, acc0, dummy_acc, acc1, vec_4b_1, vec_4b_1, mat_plus_zb6, mat_plus_zb7);     
+        AE_MULA4O4X16(acc0, dummy_acc, acc1, dummy_acc1, vec_4b_0, vec_4b_0, mat_plus_zb0, mat_plus_zb1);
+        AE_MULA4O4X16(dummy_acc, acc0, dummy_acc1, acc1, vec_4b_0, vec_4b_0, mat_plus_zb2, mat_plus_zb3);
+        AE_MULA4O4X16(acc0, dummy_acc, acc1, dummy_acc1, vec_4b_1, vec_4b_1, mat_plus_zb4, mat_plus_zb5);
+        AE_MULA4O4X16(dummy_acc, acc0, dummy_acc1, acc1, vec_4b_1, vec_4b_1, mat_plus_zb6, mat_plus_zb7);
       }
       acc = AE_ADD32S_HL_LH(acc0, acc1);
       acc = AE_ADD32S(acc, AE_MOVDA32(p_bias[vec_itr]));
diff --git a/algo/kernels/cnn/hifi5/xa_nn_transpose_conv_sym8sxasym8s.c b/algo/kernels/cnn/hifi5/xa_nn_transpose_conv_sym8sxasym8s.c
index 8cce9d8..c7517c2 100644
--- a/algo/kernels/cnn/hifi5/xa_nn_transpose_conv_sym8sxasym8s.c
+++ b/algo/kernels/cnn/hifi5/xa_nn_transpose_conv_sym8sxasym8s.c
@@ -404,7 +404,7 @@ static inline void tconv2d_sym8sxasym8s(WORD8* output_data,
     {
       acc0 = AE_ADD32(acc0, dbias0);
       ae_int16x4 out16;
-      MPY_BY_QUANT_MULT_PER_CHAN_X2X2_OUT16_ZB(out16, acc0, acc0, p_out_mult01, p_out_mult01, ls_01, ls_01, rs_01, rs_01, output_offset);
+      MPY_BY_QUANT_MULT_PER_CHAN_X2X2_OUT16_ZB(out16, acc1, acc0, p_out_mult01, p_out_mult01, ls_01, ls_01, rs_01, rs_01, output_offset);
       AE_MINMAX16(out16, min_out_16, max_out_16);
       ae_int8x8 d1 = AE_SAT8X8X16(out16, out16);
       AE_S8_0_X(d1, (ae_int8 *) pout0, 1);
@@ -449,7 +449,7 @@ static inline void tconv2d_sym8sxasym8s(WORD8* output_data,
     {
       acc1 = AE_ADD32(acc1, dbias0);
       ae_int16x4 out16;
-      MPY_BY_QUANT_MULT_X2X2_OUT16_ZB(out16, acc1, acc1, output_multiplier[out_channel], left_shift, right_shift, output_offset);
+      MPY_BY_QUANT_MULT_X2X2_OUT16_ZB(out16, acc0, acc1, output_multiplier[out_channel], left_shift, right_shift, output_offset);
       AE_MINMAX16(out16, min_out_16, max_out_16);
       ae_int8x8 d1 = AE_SAT8X8X16(out16, out16);
       AE_S8_0_I(d1, pout1, 0);
diff --git a/algo/kernels/matXvec/hifi5/xa_nn_matXvec_asym4sxasym8s.c b/algo/kernels/matXvec/hifi5/xa_nn_matXvec_asym4sxasym8s.c
index 247b590..a04651f 100644
--- a/algo/kernels/matXvec/hifi5/xa_nn_matXvec_asym4sxasym8s.c
+++ b/algo/kernels/matXvec/hifi5/xa_nn_matXvec_asym4sxasym8s.c
@@ -120,12 +120,11 @@ WORD32 xa_nn_matXvec_asym4sxasym8s_asym8s(
   XA_NNLIB_ARG_CHK_COND((rows <= 0), -1);
   XA_NNLIB_ARG_CHK_COND((cols1 <= 0), -1);
   XA_NNLIB_ARG_CHK_COND((row_stride1 < cols1), -1);
-  XA_NNLIB_ARG_CHK_COND(((row_stride1 % 2) != 0), -1);
+  XA_NNLIB_ARG_CHK_COND(((row_stride1%2!=0) && (row_stride1!=cols1)), -1);
   XA_NNLIB_ARG_CHK_COND((vec1_zero_bias < -127 || vec1_zero_bias > 128), -1);
   XA_NNLIB_ARG_CHK_COND((mat1_zero_bias < -127 || vec1_zero_bias > 128), -1);
   XA_NNLIB_ARG_CHK_COND((out_shift < -31 || out_shift > 31), -1);
   XA_NNLIB_ARG_CHK_COND((out_zero_bias < -128 || out_zero_bias > 127), -1);
-
   if(p_mat2 != NULL || p_vec2 != NULL)
   {
     return -1;
@@ -143,21 +142,36 @@ WORD32 xa_nn_matXvec_asym4sxasym8s_asym8s(
   vec_flipped_align = AE_ZALIGN128();
 
   /* Below code is for inverting order of vector by adding vector_zero_bias to the vector values. */
-  for(int vec_itr=0; vec_itr < cols1 >> 4; vec_itr++)
+  WORD32 new_cols1 = cols1;
+  if(row_stride1%2 == 1)
+  {
+    new_cols1 -= 1;
+  }
+  for(int vec_itr=0; vec_itr < new_cols1 >> 4; vec_itr++)
   {
     AE_LA8X8X2_IP(vec0, vec1, vec_align, p_vec_in);
     AE_DSEL8X8(vec0, vec1, vec0, vec1, AE_MOVINT8X8_FROMINT64(0xE6F7C4D5A2B38091));
     AE_SA8X8X2_IP(vec0, vec1, vec_flipped_align, (ae_int8x16 *)p_vec_flip_process);
   }
-  if(cols1 & 15)
+  if(new_cols1 & 15)
   {
-    AE_LAV8X8X2_XP(vec0, vec1, vec_align, p_vec_in, (cols1 & 15));
+    AE_LAV8X8X2_XP(vec0, vec1, vec_align, p_vec_in, (new_cols1 & 15));
     AE_DSEL8X8(vec0, vec1, vec0, vec1, AE_MOVINT8X8_FROMINT64(0xE6F7C4D5A2B38091));
-    AE_SA8X8X2_IP(vec0, vec1, vec_flipped_align, (ae_int8x16 *)p_vec_flip_process);
+    AE_SAV8X8X2_XP(vec0, vec1, vec_flipped_align, (ae_int8x16 *)p_vec_flip_process, (new_cols1 & 15));
+  }
+  /*converting vector = (a b c d e) to (b a d c zb e)*/
+  if(row_stride1%2 == 1)
+  {
+    new_cols1 = cols1 + 1;
+    AE_L8_IP(vec0, (ae_int8 *)p_vec_in, 1);
+    vec1 = AE_MOVDA8(-vec1_zero_bias);
+    AE_DSEL8X8(vec0, vec1, vec0, vec1, AE_MOVINT8X8_FROMINT64(0x7BFAD9C873625140));
+    AE_SAV8X8X2_XP(vec0, vec1, vec_flipped_align, (ae_int8x16 *)p_vec_flip_process, 2);
   }
   AE_SA128POS_FP(vec_flipped_align, p_vec_flip_process);
+
+  WORD32 mat_zb_x_vec = calculate_zero_point_x_vector(vec1_zero_bias, mat1_zero_bias, p_vec_flipped, new_cols1);
 
-  WORD32 mat_zb_x_vec = calculate_zero_point_x_vector(vec1_zero_bias, mat1_zero_bias, p_vec_flipped, cols1);
   int m_itr = 0, c_itr = 0;
   int left_shift;
   int right_shift;
@@ -172,9 +186,37 @@ WORD32 xa_nn_matXvec_asym4sxasym8s_asym8s(
   right_shift = out_shift>0?0:-out_shift;
 #endif
 
+  WORD32 step=2, skip=1;
+  WORD32 even_rows, odd_rows;
+
+  if(row_stride1%2 == 0)
+  {
+    step = 1;
+    skip = 2;
+    even_rows=rows;
+  }
+  else{
+    if(rows%2 == 0)
+    {
+      even_rows = rows/2;
+      odd_rows = rows/2;
+    }
+    else{
+      even_rows = rows/2 + 1;
+      odd_rows = rows/2;
+    }
+  }
+
   WORD8 *out_ptr = p_out;
+
+  ae_int8x8 dsel_hh_ll = AE_MOVINT8X8_FROMINT64(0xFBEAD9C873625140);
+  int rem_cols_shift_0, rem_cols_shift_1;
+
+  rem_cols_shift_0 = ((new_cols1 & 31) < 16) ? (64 - ((new_cols1 & 31) * 4)) : 0;
+  rem_cols_shift_1 = ((new_cols1 & 31) < 16) ? 64 : (64 - (((new_cols1 & 31)-16) * 4));
+
 
-  for(; m_itr < (rows & ~(4-1)); m_itr += 4)
+  for(; m_itr < (even_rows & ~(4-1)); m_itr += 4)
   {
     ae_int8x8 vec_zb = AE_MOVDA8(-vec1_zero_bias);
     ae_valignx2 mat_align0, mat_align1, mat_align2, mat_align3;
@@ -182,13 +224,14 @@ WORD32 xa_nn_matXvec_asym4sxasym8s_asym8s(
     ae_int32x2 acc1 = AE_MOVDA32(0);    
     if(p_bias != NULL)
     {
-      acc0 = AE_MOVDA32X2(p_bias[m_itr], p_bias[m_itr+1]);
-      acc1 = AE_MOVDA32X2(p_bias[m_itr+2], p_bias[m_itr+3]);
+
+      acc0 = AE_MOVDA32X2(p_bias[m_itr*step], p_bias[(m_itr+1)*step]);
+      acc1 = AE_MOVDA32X2(p_bias[(m_itr+2)*step], p_bias[(m_itr+3)*step]);
     }
-    ae_int8x16 *p_mat_0 = (ae_int8x16 *)(&p_mat1[(m_itr)*(row_stride1/2)*sizeof(WORD8)]);
-    ae_int8x16 *p_mat_1 = (ae_int8x16 *)(&p_mat1[(m_itr+1)*(row_stride1/2)*sizeof(WORD8)]);
-    ae_int8x16 *p_mat_2 = (ae_int8x16 *)(&p_mat1[(m_itr+2)*(row_stride1/2)*sizeof(WORD8)]);
-    ae_int8x16 *p_mat_3 = (ae_int8x16 *)(&p_mat1[(m_itr+3)*(row_stride1/2)*sizeof(WORD8)]);
+    ae_int8x16 *p_mat_0 = (ae_int8x16 *)(&p_mat1[(m_itr)*(row_stride1/skip)*sizeof(WORD8)]);
+    ae_int8x16 *p_mat_1 = (ae_int8x16 *)(&p_mat1[(m_itr+1)*(row_stride1/skip)*sizeof(WORD8)]);
+    ae_int8x16 *p_mat_2 = (ae_int8x16 *)(&p_mat1[(m_itr+2)*(row_stride1/skip)*sizeof(WORD8)]);
+    ae_int8x16 *p_mat_3 = (ae_int8x16 *)(&p_mat1[(m_itr+3)*(row_stride1/skip)*sizeof(WORD8)]);
 
     mat_align0 = AE_LA128_PP(p_mat_0);
     mat_align1 = AE_LA128_PP(p_mat_1);
@@ -203,19 +246,13 @@ WORD32 xa_nn_matXvec_asym4sxasym8s_asym8s(
     ae_int8x8 mat0_0_8b_interleaved, mat0_1_8b_interleaved, mat0_2_8b_interleaved, mat0_3_8b_interleaved, mat1_0_8b_interleaved, mat1_1_8b_interleaved, mat1_2_8b_interleaved, mat1_3_8b_interleaved;
     ae_int8x8 vec0, vec1, vec2, vec3;
     ae_int16x4 vec0_0, vec0_1, vec1_0, vec1_1, vec2_0, vec2_1, vec3_0, vec3_1;
-    ae_int8x8 dsel_hh_ll = AE_MOVINT8X8_FROMINT64(0xFBEAD9C873625140);
-    int rem_cols_shift_0, rem_cols_shift_1;
     
-    rem_cols_shift_0 = ((cols1 & 31) < 16) ? (64 - ((cols1 & 31) * 4)) : 0;
-    rem_cols_shift_1 = ((cols1 & 31) < 16) ? 64 : (64 - (((cols1 & 31)-16) * 4));
-
-    for(c_itr = 0; c_itr < cols1 >> 5; c_itr++)
+    for(c_itr = 0; c_itr < new_cols1 >> 5; c_itr++)
     {
       AE_LA8X8X2_IP(mat0_0, mat0_1, mat_align0, p_mat_0);
       AE_LA8X8X2_IP(mat1_0, mat1_1, mat_align1, p_mat_1);
       AE_LA8X8X2_IP(mat2_0, mat2_1, mat_align2, p_mat_2);
       AE_LA8X8X2_IP(mat3_0, mat3_1, mat_align3, p_mat_3);
-      AE_MOVINT4X16_FROMINT8X8_4R(mat0_0_4b, mat0_1_4b, mat1_0_4b, mat1_1_4b, mat2_0_4b, mat2_1_4b, mat3_0_4b, mat3_1_4b, mat0_0, mat0_1, mat1_0, mat1_1, mat2_0, mat2_1, mat3_0, mat3_1);
 
       AE_L8X8X2_IP(vec0, vec1, (ae_int8x16 *)p_vec_batch_0, (16 * sizeof(WORD8)));
       AE_L8X8X2_IP(vec2, vec3, (ae_int8x16 *)p_vec_batch_0, (16 * sizeof(WORD8)));
@@ -238,7 +275,7 @@ WORD32 xa_nn_matXvec_asym4sxasym8s_asym8s(
       AE_MULA8Q4X16(acc0, acc1, mat0_2_4b_interleaved, mat1_2_4b_interleaved, vec2_0, vec2_1);
       AE_MULA8Q4X16(acc0, acc1, mat0_3_4b_interleaved, mat1_3_4b_interleaved, vec3_0, vec3_1);
     }
-    if(cols1 & 31)
+    if(new_cols1 & 31)
     {
       AE_LA8X8X2_IP(mat0_0, mat0_1, mat_align0, p_mat_0);
       mat0_0 = AE_MOVINT8X8_FROMINT64(AE_SLAA64(AE_SRLA64(AE_MOVINT64_FROMINT8X8(mat0_0), rem_cols_shift_0), rem_cols_shift_0));
@@ -279,13 +316,13 @@ WORD32 xa_nn_matXvec_asym4sxasym8s_asym8s(
     ae_int16x4 out;
     MPY_BY_QUANT_MULT_SLS_X2X2_OUT16_ZB(out, acc0, acc1, out_multiplier, left_shift, right_shift, out_zero_bias)
     AE_MINMAX16(out, AE_MOVDA16(-128), AE_MOVDA16(127));
-    *out_ptr++ = (WORD8)AE_MOVAD16_3(out);
-    *out_ptr++ = (WORD8)AE_MOVAD16_2(out);
-    *out_ptr++ = (WORD8)AE_MOVAD16_1(out);
-    *out_ptr++ = (WORD8)AE_MOVAD16_0(out);
+    *out_ptr = (WORD8)AE_MOVAD16_3(out); out_ptr += step;
+    *out_ptr = (WORD8)AE_MOVAD16_2(out); out_ptr += step;
+    *out_ptr = (WORD8)AE_MOVAD16_1(out); out_ptr += step;
+    *out_ptr = (WORD8)AE_MOVAD16_0(out); out_ptr += step;
   }
 
-  for(; m_itr < (rows & ~(2-1)); m_itr += 2)
+  for(; m_itr < (even_rows & ~(2-1)); m_itr += 2)
   {
     ae_int8x8 vec_zb = AE_MOVDA8(-vec1_zero_bias);
     ae_valignx2 mat_align0, mat_align1;
@@ -293,11 +330,11 @@ WORD32 xa_nn_matXvec_asym4sxasym8s_asym8s(
     ae_int32x2 dummy = AE_MOVDA32(0);
     if(p_bias != NULL)
     {
-      acc0 = AE_MOVDA32X2(p_bias[m_itr], p_bias[m_itr+1]);
+      acc0 = AE_MOVDA32X2(p_bias[m_itr*step], p_bias[(m_itr+1)*step]);
     }
     ae_int32x2 acc1;
-    ae_int8x16 *p_mat_0 = (ae_int8x16 *)(&p_mat1[(m_itr)*(row_stride1/2)*sizeof(WORD8)]);
-    ae_int8x16 *p_mat_1 = (ae_int8x16 *)(&p_mat1[(m_itr+1)*(row_stride1/2)*sizeof(WORD8)]);
+    ae_int8x16 *p_mat_0 = (ae_int8x16 *)(&p_mat1[(m_itr)*(row_stride1/skip)*sizeof(WORD8)]);
+    ae_int8x16 *p_mat_1 = (ae_int8x16 *)(&p_mat1[(m_itr+1)*(row_stride1/skip)*sizeof(WORD8)]);
 
     mat_align0 = AE_LA128_PP(p_mat_0);
     mat_align1 = AE_LA128_PP(p_mat_1);
@@ -310,19 +347,12 @@ WORD32 xa_nn_matXvec_asym4sxasym8s_asym8s(
     ae_int8x8 mat0_0_8b_interleaved, mat0_1_8b_interleaved, mat0_2_8b_interleaved, mat0_3_8b_interleaved;
     ae_int8x8 vec0, vec1, vec2, vec3;
     ae_int16x4 vec0_0, vec0_1, vec1_0, vec1_1, vec2_0, vec2_1, vec3_0, vec3_1;
-    ae_int8x8 dsel_hh_ll = AE_MOVINT8X8_FROMINT64(0xFBEAD9C873625140);
-    int rem_cols_shift_0, rem_cols_shift_1;
     
-    rem_cols_shift_0 = ((cols1 & 31) < 16) ? (64 - ((cols1 & 31) * 4)) : 0;
-    rem_cols_shift_1 = ((cols1 & 31) < 16) ? 64 : (64 - (((cols1 & 31)-16) * 4));
-
-    for(c_itr = 0; c_itr < cols1 >> 5; c_itr++)
+    for(c_itr = 0; c_itr < new_cols1 >> 5; c_itr++)
     {
       AE_LA8X8X2_IP(mat0_0, mat0_1, mat_align0, p_mat_0);
       AE_LA8X8X2_IP(mat1_0, mat1_1, mat_align1, p_mat_1);
 
-      AE_MOVINT4X16_FROMINT8X8_2R(mat0_0_4b, mat0_1_4b, mat1_0_4b, mat1_1_4b, mat0_0, mat0_1, mat1_0, mat1_1);
-
       AE_L8X8X2_IP(vec0, vec1, (ae_int8x16 *)p_vec_batch_0, (16 * sizeof(WORD8)));
       AE_L8X8X2_IP(vec2, vec3, (ae_int8x16 *)p_vec_batch_0, (16 * sizeof(WORD8)));
 
@@ -342,7 +372,7 @@ WORD32 xa_nn_matXvec_asym4sxasym8s_asym8s(
       AE_MULA8Q4X16(acc0, acc1, mat0_2_4b_interleaved, mat0_2_4b_interleaved, vec2_0, vec2_1);
       AE_MULA8Q4X16(acc0, acc1, mat0_3_4b_interleaved, mat0_3_4b_interleaved, vec3_0, vec3_1);
     }
-    if(cols1 & 31)
+    if(new_cols1 & 31)
     {
       AE_LA8X8X2_IP(mat0_0, mat0_1, mat_align0, p_mat_0);
       mat0_0 = AE_MOVINT8X8_FROMINT64(AE_SLAA64(AE_SRLA64(AE_MOVINT64_FROMINT8X8(mat0_0), rem_cols_shift_0), rem_cols_shift_0));
@@ -350,9 +380,7 @@ WORD32 xa_nn_matXvec_asym4sxasym8s_asym8s(
       AE_LA8X8X2_IP(mat1_0, mat1_1, mat_align1, p_mat_1);
       mat1_0 = AE_MOVINT8X8_FROMINT64(AE_SLAA64(AE_SRLA64(AE_MOVINT64_FROMINT8X8(mat1_0), rem_cols_shift_0), rem_cols_shift_0));
       mat1_1 = AE_MOVINT8X8_FROMINT64(AE_SLAA64(AE_SRLA64(AE_MOVINT64_FROMINT8X8(mat1_1), rem_cols_shift_1), rem_cols_shift_1));          
-
-      AE_MOVINT4X16_FROMINT8X8_2R(mat0_0_4b, mat0_1_4b, mat1_0_4b, mat1_1_4b, mat0_0, mat0_1, mat1_0, mat1_1);
-
+
       AE_L8X8X2_IP(vec0, vec1, (ae_int8x16 *)p_vec_batch_0, (16 * sizeof(WORD8)));
       AE_L8X8X2_IP(vec2, vec3, (ae_int8x16 *)p_vec_batch_0, (16 * sizeof(WORD8)));
 
@@ -376,11 +404,11 @@ WORD32 xa_nn_matXvec_asym4sxasym8s_asym8s(
     ae_int16x4 out;
     MPY_BY_QUANT_MULT_SLS_X2X2_OUT16_ZB(out, acc0, dummy, out_multiplier, left_shift, right_shift, out_zero_bias)
     AE_MINMAX16(out, AE_MOVDA16(-128), AE_MOVDA16(127));
-    *out_ptr++ = (WORD8)AE_MOVAD16_3(out);
-    *out_ptr++ = (WORD8)AE_MOVAD16_2(out);
+    *out_ptr = (WORD8)AE_MOVAD16_3(out); out_ptr += step;
+    *out_ptr = (WORD8)AE_MOVAD16_2(out); out_ptr += step;
   }
 
-  for(; m_itr < (rows); m_itr ++)
+  for(; m_itr < (even_rows); m_itr ++)
   {
     ae_int8x8 vec_zb = AE_MOVDA8(-vec1_zero_bias);
     ae_valignx2 mat_align0;
@@ -388,10 +416,10 @@ WORD32 xa_nn_matXvec_asym4sxasym8s_asym8s(
     ae_int32x2 dummy = AE_MOVDA32(0);
     if(p_bias != NULL)
     {
-      acc0 = AE_MOVDA32X2(p_bias[m_itr], p_bias[m_itr]);
+      acc0 = AE_MOVDA32X2(p_bias[m_itr*step], p_bias[m_itr*step]);
     }
     ae_int32x2 acc1;
-    ae_int8x16 *p_mat_0 = (ae_int8x16 *)(&p_mat1[(m_itr)*(row_stride1/2)*sizeof(WORD8)]);
+    ae_int8x16 *p_mat_0 = (ae_int8x16 *)(&p_mat1[(m_itr)*(row_stride1/skip)*sizeof(WORD8)]);
 
     mat_align0 = AE_LA128_PP(p_mat_0);
 
@@ -403,17 +431,17 @@ WORD32 xa_nn_matXvec_asym4sxasym8s_asym8s(
     ae_int8x8 mat0_0_8b_interleaved, mat0_1_8b_interleaved, mat0_2_8b_interleaved, mat0_3_8b_interleaved;
     ae_int8x8 vec0, vec1, vec2, vec3;
     ae_int16x4 vec0_0, vec0_1, vec1_0, vec1_1, vec2_0, vec2_1, vec3_0, vec3_1;
-    ae_int8x8 dsel_hh_ll = AE_MOVINT8X8_FROMINT64(0xFBEAD9C873625140);
-    int rem_cols_shift_0, rem_cols_shift_1;
+    // ae_int8x8 dsel_hh_ll = AE_MOVINT8X8_FROMINT64(0xFBEAD9C873625140);
+    // int rem_cols_shift_0, rem_cols_shift_1;
     
-    rem_cols_shift_0 = ((cols1 & 31) < 16) ? (64 - ((cols1 & 31) * 4)) : 0;
-    rem_cols_shift_1 = ((cols1 & 31) < 16) ? 64 : (64 - (((cols1 & 31)-16) * 4));
+    // rem_cols_shift_0 = ((new_cols1 & 31) < 16) ? (64 - ((new_cols1 & 31) * 4)) : 0;
+    // rem_cols_shift_1 = ((new_cols1 & 31) < 16) ? 64 : (64 - (((new_cols1 & 31)-16) * 4));
 
-    for(c_itr = 0; c_itr < cols1 >> 5; c_itr++)
+    for(c_itr = 0; c_itr < new_cols1 >> 5; c_itr++)
     {
       AE_LA8X8X2_IP(mat0_0, mat0_1, mat_align0, p_mat_0);
 
-      AE_MOVINT4X16_FROMINT8X8_1R(mat0_0_4b, mat0_1_4b, mat0_0, mat0_1);
+      // AE_MOVINT4X16_FROMINT8X8_1R(mat0_0_4b, mat0_1_4b, mat0_0, mat0_1);
 
       AE_L8X8X2_IP(vec0, vec1, (ae_int8x16 *)p_vec_batch_0, (16 * sizeof(WORD8)));
       AE_L8X8X2_IP(vec2, vec3, (ae_int8x16 *)p_vec_batch_0, (16 * sizeof(WORD8)));
@@ -434,13 +462,13 @@ WORD32 xa_nn_matXvec_asym4sxasym8s_asym8s(
       AE_MULA8Q4X16(acc0, acc1, mat0_2_4b_interleaved, mat0_2_4b_interleaved, vec2_0, vec2_1);
       AE_MULA8Q4X16(acc0, acc1, mat0_3_4b_interleaved, mat0_3_4b_interleaved, vec3_0, vec3_1);
     }
-    if(cols1 & 31)
+    if(new_cols1 & 31)
     {
       AE_LA8X8X2_IP(mat0_0, mat0_1, mat_align0, p_mat_0);
       mat0_0 = AE_MOVINT8X8_FROMINT64(AE_SLAA64(AE_SRLA64(AE_MOVINT64_FROMINT8X8(mat0_0), rem_cols_shift_0), rem_cols_shift_0));
       mat0_1 = AE_MOVINT8X8_FROMINT64(AE_SLAA64(AE_SRLA64(AE_MOVINT64_FROMINT8X8(mat0_1), rem_cols_shift_1), rem_cols_shift_1));                 
 
-      AE_MOVINT4X16_FROMINT8X8_1R(mat0_0_4b, mat0_1_4b, mat0_0, mat0_1);
+      // AE_MOVINT4X16_FROMINT8X8_1R(mat0_0_4b, mat0_1_4b, mat0_0, mat0_1);
 
       AE_L8X8X2_IP(vec0, vec1, (ae_int8x16 *)p_vec_batch_0, (16 * sizeof(WORD8)));
       AE_L8X8X2_IP(vec2, vec3, (ae_int8x16 *)p_vec_batch_0, (16 * sizeof(WORD8)));
@@ -465,7 +493,322 @@ WORD32 xa_nn_matXvec_asym4sxasym8s_asym8s(
     ae_int16x4 out;
     MPY_BY_QUANT_MULT_SLS_X2X2_OUT16_ZB(out, acc0, dummy, out_multiplier, left_shift, right_shift, out_zero_bias)
     AE_MINMAX16(out, AE_MOVDA16(-128), AE_MOVDA16(127));
-    *out_ptr++ = (WORD8)AE_MOVAD16_3(out);
+    *out_ptr = (WORD8)AE_MOVAD16_3(out); out_ptr += step;
+  }
+
+  //This part of the code will run only for rows at odd idx
+  if(row_stride1%2 == 1)
+  {
+    p_vec_flip_process = (WORD8 *)p_vec_flipped;
+    p_vec_in = (ae_int8x16 *)p_vec1;
+
+    ae_int8x8 vec_out0, vec_out1;
+    vec_flipped_align = AE_ZALIGN128();
+
+    /* Below code is for inverting order of vector by adding vector_zero_bias to the vector values. */
+
+    /*converting vector = (a b c d e) to (a zb c b e d)*/
+    ae_int8x8 vzb8x8 = AE_MOVDA8(-vec1_zero_bias);
+    AE_L8_IP(vec0, (ae_int8 *)p_vec_in, 1);
+    AE_DSEL8X8(vec_out0, vec_out1, vzb8x8, vec0, AE_MOVINT8X8_FROMINT64(0x7BFAD9C873625140));
+    AE_SAV8X8X2_XP(vec_out0, vec_out1, vec_flipped_align, (ae_int8x16 *)p_vec_flip_process, 2);
+
+    vec_align = AE_LA128_PP(p_vec_in);
+    new_cols1 = cols1 - 1;
+
+    for(int vec_itr=0; vec_itr < new_cols1 >> 4; vec_itr++)
+    {
+      AE_LA8X8X2_IP(vec0, vec1, vec_align, p_vec_in);
+      AE_DSEL8X8(vec0, vec1, vec0, vec1, AE_MOVINT8X8_FROMINT64(0xE6F7C4D5A2B38091));
+      AE_SA8X8X2_IP(vec0, vec1, vec_flipped_align, (ae_int8x16 *)p_vec_flip_process);
+    }
+    if(new_cols1 & 15)
+    {
+      AE_LAV8X8X2_XP(vec0, vec1, vec_align, p_vec_in, (new_cols1 & 15));
+      AE_DSEL8X8(vec0, vec1, vec0, vec1, AE_MOVINT8X8_FROMINT64(0xE6F7C4D5A2B38091));
+      AE_SA8X8X2_IP(vec0, vec1, vec_flipped_align, (ae_int8x16 *)p_vec_flip_process);
+    }
+    AE_SA128POS_FP(vec_flipped_align, p_vec_flip_process);
+    new_cols1 = cols1 + 1;
+    mat_zb_x_vec = calculate_zero_point_x_vector(vec1_zero_bias, mat1_zero_bias, p_vec_flipped, new_cols1);
+    WORD8 *out_ptr = p_out;
+    out_ptr++;
+    m_itr = 0;
+
+    for(; m_itr < (odd_rows & ~(4-1)); m_itr += 4)
+    {
+      ae_int8x8 vec_zb = AE_MOVDA8(-vec1_zero_bias);
+      ae_valignx2 mat_align0, mat_align1, mat_align2, mat_align3;
+      ae_int32x2 acc0 = AE_MOVDA32(0);
+      ae_int32x2 acc1 = AE_MOVDA32(0);
+      if(p_bias != NULL)
+      {
+        acc0 = AE_MOVDA32X2(p_bias[m_itr*step+1], p_bias[(m_itr+1)*step+1]);
+        acc1 = AE_MOVDA32X2(p_bias[(m_itr+2)*step+1], p_bias[(m_itr+3)*step+1]);
+      }
+      ae_int8x16 *p_mat_0 = (ae_int8x16 *)(&p_mat1[(m_itr)*(row_stride1)*sizeof(WORD8) + (row_stride1/2)]);
+      ae_int8x16 *p_mat_1 = (ae_int8x16 *)(&p_mat1[(m_itr+1)*(row_stride1)*sizeof(WORD8) + (row_stride1/2)]);
+      ae_int8x16 *p_mat_2 = (ae_int8x16 *)(&p_mat1[(m_itr+2)*(row_stride1)*sizeof(WORD8) + (row_stride1/2)]);
+      ae_int8x16 *p_mat_3 = (ae_int8x16 *)(&p_mat1[(m_itr+3)*(row_stride1)*sizeof(WORD8) + (row_stride1/2)]);
+
+      mat_align0 = AE_LA128_PP(p_mat_0);
+      mat_align1 = AE_LA128_PP(p_mat_1);
+      mat_align2 = AE_LA128_PP(p_mat_2);
+      mat_align3 = AE_LA128_PP(p_mat_3);
+
+      WORD8 *p_vec_batch_0  = (WORD8 *)p_vec_flipped;
+
+      ae_int8x8 mat0_0, mat0_1, mat1_0, mat1_1, mat2_0, mat2_1, mat3_0, mat3_1;
+      ae_int4x16 mat0_0_4b, mat0_1_4b, mat1_0_4b, mat1_1_4b, mat2_0_4b, mat2_1_4b, mat3_0_4b, mat3_1_4b;
+      ae_int4x16 mat0_0_4b_interleaved, mat0_1_4b_interleaved, mat0_2_4b_interleaved, mat0_3_4b_interleaved, mat1_0_4b_interleaved, mat1_1_4b_interleaved, mat1_2_4b_interleaved, mat1_3_4b_interleaved;
+      ae_int8x8 mat0_0_8b_interleaved, mat0_1_8b_interleaved, mat0_2_8b_interleaved, mat0_3_8b_interleaved, mat1_0_8b_interleaved, mat1_1_8b_interleaved, mat1_2_8b_interleaved, mat1_3_8b_interleaved;
+      ae_int8x8 vec0, vec1, vec2, vec3;
+      ae_int16x4 vec0_0, vec0_1, vec1_0, vec1_1, vec2_0, vec2_1, vec3_0, vec3_1;
+
+      for(c_itr = 0; c_itr < new_cols1 >> 5; c_itr++)
+      {
+        AE_LA8X8X2_IP(mat0_0, mat0_1, mat_align0, p_mat_0);
+        AE_LA8X8X2_IP(mat1_0, mat1_1, mat_align1, p_mat_1);
+        AE_LA8X8X2_IP(mat2_0, mat2_1, mat_align2, p_mat_2);
+        AE_LA8X8X2_IP(mat3_0, mat3_1, mat_align3, p_mat_3);
+        AE_MOVINT4X16_FROMINT8X8_4R(mat0_0_4b, mat0_1_4b, mat1_0_4b, mat1_1_4b, mat2_0_4b, mat2_1_4b, mat3_0_4b, mat3_1_4b, mat0_0, mat0_1, mat1_0, mat1_1, mat2_0, mat2_1, mat3_0, mat3_1);
+
+        AE_L8X8X2_IP(vec0, vec1, (ae_int8x16 *)p_vec_batch_0, (16 * sizeof(WORD8)));
+        AE_L8X8X2_IP(vec2, vec3, (ae_int8x16 *)p_vec_batch_0, (16 * sizeof(WORD8)));
+
+        AE_SUBW8(vec0_0, vec0_1, vec0, vec_zb);
+        AE_SUBW8(vec1_0, vec1_1, vec1, vec_zb);
+        AE_SUBW8(vec2_0, vec2_1, vec2, vec_zb);
+        AE_SUBW8(vec3_0, vec3_1, vec3, vec_zb);
+
+        AE_MOVINT4X16_FROMINT8X8_4R(mat0_0_4b, mat0_1_4b, mat1_0_4b, mat1_1_4b, mat2_0_4b, mat2_1_4b, mat3_0_4b, mat3_1_4b, mat0_0, mat0_1, mat1_0, mat1_1, mat2_0, mat2_1, mat3_0, mat3_1);
+
+        AE_DSEL8X8(mat0_0_8b_interleaved, mat0_1_8b_interleaved, AE_MOVINT8X8_FROMINT4X16(mat0_0_4b), AE_MOVINT8X8_FROMINT4X16(mat1_0_4b), dsel_hh_ll);
+        AE_DSEL8X8(mat0_2_8b_interleaved, mat0_3_8b_interleaved, AE_MOVINT8X8_FROMINT4X16(mat0_1_4b), AE_MOVINT8X8_FROMINT4X16(mat1_1_4b), dsel_hh_ll);
+        AE_DSEL8X8(mat1_0_8b_interleaved, mat1_1_8b_interleaved, AE_MOVINT8X8_FROMINT4X16(mat2_0_4b), AE_MOVINT8X8_FROMINT4X16(mat3_0_4b), dsel_hh_ll);
+        AE_DSEL8X8(mat1_2_8b_interleaved, mat1_3_8b_interleaved, AE_MOVINT8X8_FROMINT4X16(mat2_1_4b), AE_MOVINT8X8_FROMINT4X16(mat3_1_4b), dsel_hh_ll);
+
+        AE_MOVINT4X16_FROMINT8X8_4R(mat0_0_4b_interleaved, mat0_1_4b_interleaved, mat0_2_4b_interleaved, mat0_3_4b_interleaved, mat1_0_4b_interleaved, mat1_1_4b_interleaved, mat1_2_4b_interleaved, mat1_3_4b_interleaved, mat0_0_8b_interleaved, mat0_1_8b_interleaved, mat0_2_8b_interleaved, mat0_3_8b_interleaved, mat1_0_8b_interleaved, mat1_1_8b_interleaved, mat1_2_8b_interleaved, mat1_3_8b_interleaved);
+        AE_MULA8Q4X16(acc0, acc1, mat0_0_4b_interleaved, mat1_0_4b_interleaved, vec0_0, vec0_1);
+        AE_MULA8Q4X16(acc0, acc1, mat0_1_4b_interleaved, mat1_1_4b_interleaved, vec1_0, vec1_1);
+        AE_MULA8Q4X16(acc0, acc1, mat0_2_4b_interleaved, mat1_2_4b_interleaved, vec2_0, vec2_1);
+        AE_MULA8Q4X16(acc0, acc1, mat0_3_4b_interleaved, mat1_3_4b_interleaved, vec3_0, vec3_1);
+      }
+      if(new_cols1 & 31)
+      {
+        AE_LA8X8X2_IP(mat0_0, mat0_1, mat_align0, p_mat_0);
+        mat0_0 = AE_MOVINT8X8_FROMINT64(AE_SLAA64(AE_SRLA64(AE_MOVINT64_FROMINT8X8(mat0_0), rem_cols_shift_0), rem_cols_shift_0));
+        mat0_1 = AE_MOVINT8X8_FROMINT64(AE_SLAA64(AE_SRLA64(AE_MOVINT64_FROMINT8X8(mat0_1), rem_cols_shift_1), rem_cols_shift_1));
+        AE_LA8X8X2_IP(mat1_0, mat1_1, mat_align1, p_mat_1);
+        mat1_0 = AE_MOVINT8X8_FROMINT64(AE_SLAA64(AE_SRLA64(AE_MOVINT64_FROMINT8X8(mat1_0), rem_cols_shift_0), rem_cols_shift_0));
+        mat1_1 = AE_MOVINT8X8_FROMINT64(AE_SLAA64(AE_SRLA64(AE_MOVINT64_FROMINT8X8(mat1_1), rem_cols_shift_1), rem_cols_shift_1));
+        AE_LA8X8X2_IP(mat2_0, mat2_1, mat_align2, p_mat_2);
+        mat2_0 = AE_MOVINT8X8_FROMINT64(AE_SLAA64(AE_SRLA64(AE_MOVINT64_FROMINT8X8(mat2_0), rem_cols_shift_0), rem_cols_shift_0));
+        mat2_1 = AE_MOVINT8X8_FROMINT64(AE_SLAA64(AE_SRLA64(AE_MOVINT64_FROMINT8X8(mat2_1), rem_cols_shift_1), rem_cols_shift_1));
+        AE_LA8X8X2_IP(mat3_0, mat3_1, mat_align3, p_mat_3);
+        mat3_0 = AE_MOVINT8X8_FROMINT64(AE_SLAA64(AE_SRLA64(AE_MOVINT64_FROMINT8X8(mat3_0), rem_cols_shift_0), rem_cols_shift_0));
+        mat3_1 = AE_MOVINT8X8_FROMINT64(AE_SLAA64(AE_SRLA64(AE_MOVINT64_FROMINT8X8(mat3_1), rem_cols_shift_1), rem_cols_shift_1));
+
+        AE_MOVINT4X16_FROMINT8X8_4R(mat0_0_4b, mat0_1_4b, mat1_0_4b, mat1_1_4b, mat2_0_4b, mat2_1_4b, mat3_0_4b, mat3_1_4b, mat0_0, mat0_1, mat1_0, mat1_1, mat2_0, mat2_1, mat3_0, mat3_1);
+
+        AE_DSEL8X8(mat0_0_8b_interleaved, mat0_1_8b_interleaved, AE_MOVINT8X8_FROMINT4X16(mat0_0_4b), AE_MOVINT8X8_FROMINT4X16(mat1_0_4b), dsel_hh_ll);
+        AE_DSEL8X8(mat0_2_8b_interleaved, mat0_3_8b_interleaved, AE_MOVINT8X8_FROMINT4X16(mat0_1_4b), AE_MOVINT8X8_FROMINT4X16(mat1_1_4b), dsel_hh_ll);
+        AE_DSEL8X8(mat1_0_8b_interleaved, mat1_1_8b_interleaved, AE_MOVINT8X8_FROMINT4X16(mat2_0_4b), AE_MOVINT8X8_FROMINT4X16(mat3_0_4b), dsel_hh_ll);
+        AE_DSEL8X8(mat1_2_8b_interleaved, mat1_3_8b_interleaved, AE_MOVINT8X8_FROMINT4X16(mat2_1_4b), AE_MOVINT8X8_FROMINT4X16(mat3_1_4b), dsel_hh_ll);
+
+        AE_MOVINT4X16_FROMINT8X8_4R(mat0_0_4b_interleaved, mat0_1_4b_interleaved, mat0_2_4b_interleaved, mat0_3_4b_interleaved, mat1_0_4b_interleaved, mat1_1_4b_interleaved, mat1_2_4b_interleaved, mat1_3_4b_interleaved, mat0_0_8b_interleaved, mat0_1_8b_interleaved, mat0_2_8b_interleaved, mat0_3_8b_interleaved, mat1_0_8b_interleaved, mat1_1_8b_interleaved, mat1_2_8b_interleaved, mat1_3_8b_interleaved);
+        AE_L8X8X2_IP(vec0, vec1, (ae_int8x16 *)p_vec_batch_0, (16 * sizeof(WORD8)));
+        AE_L8X8X2_IP(vec2, vec3, (ae_int8x16 *)p_vec_batch_0, (16 * sizeof(WORD8)));
+
+        AE_SUBW8(vec0_0, vec0_1, vec0, vec_zb);
+        AE_SUBW8(vec1_0, vec1_1, vec1, vec_zb);
+        AE_SUBW8(vec2_0, vec2_1, vec2, vec_zb);
+        AE_SUBW8(vec3_0, vec3_1, vec3, vec_zb);
+
+        AE_MULA8Q4X16(acc0, acc1, mat0_0_4b_interleaved, mat1_0_4b_interleaved, vec0_0, vec0_1);
+        AE_MULA8Q4X16(acc0, acc1, mat0_1_4b_interleaved, mat1_1_4b_interleaved, vec1_0, vec1_1);
+        AE_MULA8Q4X16(acc0, acc1, mat0_2_4b_interleaved, mat1_2_4b_interleaved, vec2_0, vec2_1);
+        AE_MULA8Q4X16(acc0, acc1, mat0_3_4b_interleaved, mat1_3_4b_interleaved, vec3_0, vec3_1);
+      }
+      acc0 = AE_ADD32S(acc0, AE_MOVDA32(mat_zb_x_vec));
+      acc1 = AE_ADD32S(acc1, AE_MOVDA32(mat_zb_x_vec));
+      ae_int16x4 out;
+      MPY_BY_QUANT_MULT_SLS_X2X2_OUT16_ZB(out, acc0, acc1, out_multiplier, left_shift, right_shift, out_zero_bias)
+      AE_MINMAX16(out, AE_MOVDA16(-128), AE_MOVDA16(127));
+      *out_ptr = (WORD8)AE_MOVAD16_3(out); out_ptr += step;
+      *out_ptr = (WORD8)AE_MOVAD16_2(out); out_ptr += step;
+      *out_ptr = (WORD8)AE_MOVAD16_1(out); out_ptr += step;
+      *out_ptr = (WORD8)AE_MOVAD16_0(out); out_ptr += step;
+    }
+
+    for(; m_itr < (odd_rows & ~(2-1)); m_itr += 2)
+    {
+      ae_int8x8 vec_zb = AE_MOVDA8(-vec1_zero_bias);
+      ae_valignx2 mat_align0, mat_align1;
+      ae_int32x2 acc0 = AE_MOVDA32(0);
+      ae_int32x2 dummy = AE_MOVDA32(0);
+      if(p_bias != NULL)
+      {
+        acc0 = AE_MOVDA32X2(p_bias[(m_itr)*step+1], p_bias[(m_itr+1)*step+1]);
+      }
+      ae_int32x2 acc1;
+      ae_int8x16 *p_mat_0 = (ae_int8x16 *)(&p_mat1[(m_itr)*(row_stride1)*sizeof(WORD8) + (row_stride1/2)]);
+      ae_int8x16 *p_mat_1 = (ae_int8x16 *)(&p_mat1[(m_itr+1)*(row_stride1)*sizeof(WORD8) + (row_stride1/2)]);
+
+      mat_align0 = AE_LA128_PP(p_mat_0);
+      mat_align1 = AE_LA128_PP(p_mat_1);
+      WORD8 *p_vec_batch_0  = (WORD8 *)p_vec_flipped;
+
+      ae_int8x8 mat0_0, mat0_1, mat1_0, mat1_1;
+      ae_int4x16 mat0_0_4b, mat0_1_4b, mat1_0_4b, mat1_1_4b;
+      ae_int4x16 mat0_0_4b_interleaved, mat0_1_4b_interleaved, mat0_2_4b_interleaved, mat0_3_4b_interleaved;
+      ae_int8x8 mat0_0_8b_interleaved, mat0_1_8b_interleaved, mat0_2_8b_interleaved, mat0_3_8b_interleaved;
+      ae_int8x8 vec0, vec1, vec2, vec3;
+      ae_int16x4 vec0_0, vec0_1, vec1_0, vec1_1, vec2_0, vec2_1, vec3_0, vec3_1;
+
+      for(c_itr = 0; c_itr < new_cols1 >> 5; c_itr++)
+      {
+        AE_LA8X8X2_IP(mat0_0, mat0_1, mat_align0, p_mat_0);
+        AE_LA8X8X2_IP(mat1_0, mat1_1, mat_align1, p_mat_1);
+
+        AE_L8X8X2_IP(vec0, vec1, (ae_int8x16 *)p_vec_batch_0, (16 * sizeof(WORD8)));
+        AE_L8X8X2_IP(vec2, vec3, (ae_int8x16 *)p_vec_batch_0, (16 * sizeof(WORD8)));
+
+        AE_SUBW8(vec0_0, vec0_1, vec0, vec_zb);
+        AE_SUBW8(vec1_0, vec1_1, vec1, vec_zb);
+        AE_SUBW8(vec2_0, vec2_1, vec2, vec_zb);
+        AE_SUBW8(vec3_0, vec3_1, vec3, vec_zb);
+
+        AE_MOVINT4X16_FROMINT8X8_2R(mat0_0_4b, mat0_1_4b, mat1_0_4b, mat1_1_4b, mat0_0, mat0_1, mat1_0, mat1_1);
+
+        AE_DSEL8X8(mat0_0_8b_interleaved, mat0_1_8b_interleaved, AE_MOVINT8X8_FROMINT4X16(mat0_0_4b), AE_MOVINT8X8_FROMINT4X16(mat1_0_4b), dsel_hh_ll);
+        AE_DSEL8X8(mat0_2_8b_interleaved, mat0_3_8b_interleaved, AE_MOVINT8X8_FROMINT4X16(mat0_1_4b), AE_MOVINT8X8_FROMINT4X16(mat1_1_4b), dsel_hh_ll);
+
+        AE_MOVINT4X16_FROMINT8X8_2R(mat0_0_4b_interleaved, mat0_1_4b_interleaved, mat0_2_4b_interleaved, mat0_3_4b_interleaved, mat0_0_8b_interleaved, mat0_1_8b_interleaved, mat0_2_8b_interleaved, mat0_3_8b_interleaved);
+        AE_MULA8Q4X16(acc0, acc1, mat0_0_4b_interleaved, mat0_0_4b_interleaved, vec0_0, vec0_1);
+        AE_MULA8Q4X16(acc0, acc1, mat0_1_4b_interleaved, mat0_1_4b_interleaved, vec1_0, vec1_1);
+        AE_MULA8Q4X16(acc0, acc1, mat0_2_4b_interleaved, mat0_2_4b_interleaved, vec2_0, vec2_1);
+        AE_MULA8Q4X16(acc0, acc1, mat0_3_4b_interleaved, mat0_3_4b_interleaved, vec3_0, vec3_1);
+      }
+      if(new_cols1 & 31)
+      {
+        AE_LA8X8X2_IP(mat0_0, mat0_1, mat_align0, p_mat_0);
+        mat0_0 = AE_MOVINT8X8_FROMINT64(AE_SLAA64(AE_SRLA64(AE_MOVINT64_FROMINT8X8(mat0_0), rem_cols_shift_0), rem_cols_shift_0));
+        mat0_1 = AE_MOVINT8X8_FROMINT64(AE_SLAA64(AE_SRLA64(AE_MOVINT64_FROMINT8X8(mat0_1), rem_cols_shift_1), rem_cols_shift_1));
+        AE_LA8X8X2_IP(mat1_0, mat1_1, mat_align1, p_mat_1);
+        mat1_0 = AE_MOVINT8X8_FROMINT64(AE_SLAA64(AE_SRLA64(AE_MOVINT64_FROMINT8X8(mat1_0), rem_cols_shift_0), rem_cols_shift_0));
+        mat1_1 = AE_MOVINT8X8_FROMINT64(AE_SLAA64(AE_SRLA64(AE_MOVINT64_FROMINT8X8(mat1_1), rem_cols_shift_1), rem_cols_shift_1));
+
+        AE_L8X8X2_IP(vec0, vec1, (ae_int8x16 *)p_vec_batch_0, (16 * sizeof(WORD8)));
+        AE_L8X8X2_IP(vec2, vec3, (ae_int8x16 *)p_vec_batch_0, (16 * sizeof(WORD8)));
+        AE_SUBW8(vec0_0, vec0_1, vec0, vec_zb);
+        AE_SUBW8(vec1_0, vec1_1, vec1, vec_zb);
+        AE_SUBW8(vec2_0, vec2_1, vec2, vec_zb);
+        AE_SUBW8(vec3_0, vec3_1, vec3, vec_zb);
+
+        AE_MOVINT4X16_FROMINT8X8_2R(mat0_0_4b, mat0_1_4b, mat1_0_4b, mat1_1_4b, mat0_0, mat0_1, mat1_0, mat1_1);
+
+        AE_DSEL8X8(mat0_0_8b_interleaved, mat0_1_8b_interleaved, AE_MOVINT8X8_FROMINT4X16(mat0_0_4b), AE_MOVINT8X8_FROMINT4X16(mat1_0_4b), dsel_hh_ll);
+        AE_DSEL8X8(mat0_2_8b_interleaved, mat0_3_8b_interleaved, AE_MOVINT8X8_FROMINT4X16(mat0_1_4b), AE_MOVINT8X8_FROMINT4X16(mat1_1_4b), dsel_hh_ll);
+
+        AE_MOVINT4X16_FROMINT8X8_2R(mat0_0_4b_interleaved, mat0_1_4b_interleaved, mat0_2_4b_interleaved, mat0_3_4b_interleaved, mat0_0_8b_interleaved, mat0_1_8b_interleaved, mat0_2_8b_interleaved, mat0_3_8b_interleaved);
+        AE_MULA8Q4X16(acc0, acc1, mat0_0_4b_interleaved, mat0_0_4b_interleaved, vec0_0, vec0_1);
+        AE_MULA8Q4X16(acc0, acc1, mat0_1_4b_interleaved, mat0_1_4b_interleaved, vec1_0, vec1_1);
+        AE_MULA8Q4X16(acc0, acc1, mat0_2_4b_interleaved, mat0_2_4b_interleaved, vec2_0, vec2_1);
+        AE_MULA8Q4X16(acc0, acc1, mat0_3_4b_interleaved, mat0_3_4b_interleaved, vec3_0, vec3_1);
+      }
+      acc0 = AE_ADD32S(acc0, AE_MOVDA32(mat_zb_x_vec));
+      ae_int16x4 out;
+      MPY_BY_QUANT_MULT_SLS_X2X2_OUT16_ZB(out, acc0, dummy, out_multiplier, left_shift, right_shift, out_zero_bias)
+      AE_MINMAX16(out, AE_MOVDA16(-128), AE_MOVDA16(127));
+      *out_ptr = (WORD8)AE_MOVAD16_3(out); out_ptr += step;
+      *out_ptr = (WORD8)AE_MOVAD16_2(out); out_ptr += step;
+    }
+    for(; m_itr < (odd_rows); m_itr ++)
+    {
+      ae_int8x8 vec_zb = AE_MOVDA8(-vec1_zero_bias);
+      ae_valignx2 mat_align0;
+      ae_int32x2 acc0 = AE_MOVDA32(0);
+      ae_int32x2 dummy = AE_MOVDA32(0);
+      if(p_bias != NULL)
+      {
+        acc0 = AE_MOVDA32X2(p_bias[m_itr*step+1], p_bias[m_itr*step+1]);
+      }
+      ae_int32x2 acc1;
+      ae_int8x16 *p_mat_0 = (ae_int8x16 *)(&p_mat1[(m_itr)*(row_stride1)*sizeof(WORD8) + (row_stride1/2)]);
+
+      mat_align0 = AE_LA128_PP(p_mat_0);
+      WORD8 *p_vec_batch_0  = (WORD8 *)p_vec_flipped;
+
+      ae_int8x8 mat0_0, mat0_1;
+      ae_int4x16 mat0_0_4b, mat0_1_4b;
+      ae_int4x16 mat0_0_4b_interleaved, mat0_1_4b_interleaved, mat0_2_4b_interleaved, mat0_3_4b_interleaved;
+      ae_int8x8 mat0_0_8b_interleaved, mat0_1_8b_interleaved, mat0_2_8b_interleaved, mat0_3_8b_interleaved;
+      ae_int8x8 vec0, vec1, vec2, vec3;
+      ae_int16x4 vec0_0, vec0_1, vec1_0, vec1_1, vec2_0, vec2_1, vec3_0, vec3_1;
+      ae_int8x8 dsel_hh_ll = AE_MOVINT8X8_FROMINT64(0xFBEAD9C873625140);
+      int rem_cols_shift_0, rem_cols_shift_1;
+
+      rem_cols_shift_0 = ((new_cols1 & 31) < 16) ? (64 - ((new_cols1 & 31) * 4)) : 0;
+      rem_cols_shift_1 = ((new_cols1 & 31) < 16) ? 64 : (64 - (((new_cols1 & 31)-16) * 4));
+
+      for(c_itr = 0; c_itr < new_cols1 >> 5; c_itr++)
+      {
+        AE_LA8X8X2_IP(mat0_0, mat0_1, mat_align0, p_mat_0);
+
+        AE_L8X8X2_IP(vec0, vec1, (ae_int8x16 *)p_vec_batch_0, (16 * sizeof(WORD8)));
+        AE_L8X8X2_IP(vec2, vec3, (ae_int8x16 *)p_vec_batch_0, (16 * sizeof(WORD8)));
+
+        AE_SUBW8(vec0_0, vec0_1, vec0, vec_zb);
+        AE_SUBW8(vec1_0, vec1_1, vec1, vec_zb);
+        AE_SUBW8(vec2_0, vec2_1, vec2, vec_zb);
+        AE_SUBW8(vec3_0, vec3_1, vec3, vec_zb);
+
+        AE_MOVINT4X16_FROMINT8X8_1R(mat0_0_4b, mat0_1_4b, mat0_0, mat0_1);
+
+        AE_DSEL8X8(mat0_0_8b_interleaved, mat0_1_8b_interleaved, AE_MOVINT8X8_FROMINT4X16(mat0_0_4b), AE_MOVINT8X8_FROMINT4X16(mat0_0_4b), dsel_hh_ll);
+        AE_DSEL8X8(mat0_2_8b_interleaved, mat0_3_8b_interleaved, AE_MOVINT8X8_FROMINT4X16(mat0_1_4b), AE_MOVINT8X8_FROMINT4X16(mat0_1_4b), dsel_hh_ll);
+
+        AE_MOVINT4X16_FROMINT8X8_2R(mat0_0_4b_interleaved, mat0_1_4b_interleaved, mat0_2_4b_interleaved, mat0_3_4b_interleaved, mat0_0_8b_interleaved, mat0_1_8b_interleaved, mat0_2_8b_interleaved, mat0_3_8b_interleaved);
+        AE_MULA8Q4X16(acc0, acc1, mat0_0_4b_interleaved, mat0_0_4b_interleaved, vec0_0, vec0_1);
+        AE_MULA8Q4X16(acc0, acc1, mat0_1_4b_interleaved, mat0_1_4b_interleaved, vec1_0, vec1_1);
+        AE_MULA8Q4X16(acc0, acc1, mat0_2_4b_interleaved, mat0_2_4b_interleaved, vec2_0, vec2_1);
+        AE_MULA8Q4X16(acc0, acc1, mat0_3_4b_interleaved, mat0_3_4b_interleaved, vec3_0, vec3_1);
+      }
+
+      if(new_cols1 & 31)
+      {
+        AE_LA8X8X2_IP(mat0_0, mat0_1, mat_align0, p_mat_0);
+        mat0_0 = AE_MOVINT8X8_FROMINT64(AE_SLAA64(AE_SRLA64(AE_MOVINT64_FROMINT8X8(mat0_0), rem_cols_shift_0), rem_cols_shift_0));
+        mat0_1 = AE_MOVINT8X8_FROMINT64(AE_SLAA64(AE_SRLA64(AE_MOVINT64_FROMINT8X8(mat0_1), rem_cols_shift_1), rem_cols_shift_1));
+
+        AE_L8X8X2_IP(vec0, vec1, (ae_int8x16 *)p_vec_batch_0, (16 * sizeof(WORD8)));
+        AE_L8X8X2_IP(vec2, vec3, (ae_int8x16 *)p_vec_batch_0, (16 * sizeof(WORD8)));
+
+        AE_SUBW8(vec0_0, vec0_1, vec0, vec_zb);
+        AE_SUBW8(vec1_0, vec1_1, vec1, vec_zb);
+        AE_SUBW8(vec2_0, vec2_1, vec2, vec_zb);
+        AE_SUBW8(vec3_0, vec3_1, vec3, vec_zb);
+
+        AE_MOVINT4X16_FROMINT8X8_1R(mat0_0_4b, mat0_1_4b, mat0_0, mat0_1);
+
+        AE_DSEL8X8(mat0_0_8b_interleaved, mat0_1_8b_interleaved, AE_MOVINT8X8_FROMINT4X16(mat0_0_4b), AE_MOVINT8X8_FROMINT4X16(mat0_0_4b), dsel_hh_ll);
+        AE_DSEL8X8(mat0_2_8b_interleaved, mat0_3_8b_interleaved, AE_MOVINT8X8_FROMINT4X16(mat0_1_4b), AE_MOVINT8X8_FROMINT4X16(mat0_1_4b), dsel_hh_ll);
+
+        AE_MOVINT4X16_FROMINT8X8_2R(mat0_0_4b_interleaved, mat0_1_4b_interleaved, mat0_2_4b_interleaved, mat0_3_4b_interleaved, mat0_0_8b_interleaved, mat0_1_8b_interleaved, mat0_2_8b_interleaved, mat0_3_8b_interleaved);
+        AE_MULA8Q4X16(acc0, acc1, mat0_0_4b_interleaved, mat0_0_4b_interleaved, vec0_0, vec0_1);
+        AE_MULA8Q4X16(acc0, acc1, mat0_1_4b_interleaved, mat0_1_4b_interleaved, vec1_0, vec1_1);
+        AE_MULA8Q4X16(acc0, acc1, mat0_2_4b_interleaved, mat0_2_4b_interleaved, vec2_0, vec2_1);
+        AE_MULA8Q4X16(acc0, acc1, mat0_3_4b_interleaved, mat0_3_4b_interleaved, vec3_0, vec3_1);
+      }
+      acc0 = AE_ADD32S(acc0, AE_MOVDA32(mat_zb_x_vec));
+      ae_int16x4 out;
+      MPY_BY_QUANT_MULT_SLS_X2X2_OUT16_ZB(out, acc0, dummy, out_multiplier, left_shift, right_shift, out_zero_bias)
+      AE_MINMAX16(out, AE_MOVDA16(-128), AE_MOVDA16(127));
+      *out_ptr = (WORD8)AE_MOVAD16_3(out); out_ptr += step;
+    }
+
+
   }
   
   return 0;  
diff --git a/algo/kernels/matXvec/hifi5/xa_nn_matmul_asym8sxasym8s.c b/algo/kernels/matXvec/hifi5/xa_nn_matmul_asym8sxasym8s.c
index 3b646e0..5e2249e 100644
--- a/algo/kernels/matXvec/hifi5/xa_nn_matmul_asym8sxasym8s.c
+++ b/algo/kernels/matXvec/hifi5/xa_nn_matmul_asym8sxasym8s.c
@@ -1688,7 +1688,7 @@ WORD32 xa_nn_matmul_v2_asym8sxasym8s_asym8s(
 
         ae_int16x4 out_0;
 
-        MPY_BY_QUANT_MULT_X2X2_OUT16_ZB(out_0, acc_row0_vec0, acc_row0_vec0, out_multiplier, left_shift, right_shift, out_zero_bias);
+        MPY_BY_QUANT_MULT_X2X2_OUT16_ZB(out_0, acc_row0_vec0, acc_row0_vec1, out_multiplier, left_shift, right_shift, out_zero_bias);
         AE_MINMAX16(out_0, AE_MOVDA16(out_activation_min), AE_MOVDA16(out_activation_max));
 
         ae_int8x8 temp_vec0;
@@ -2173,15 +2173,16 @@ WORD32 xa_nn_matmul_asym8sxasym8s_asym8s(
       for(vec_itr = 0; vec_itr < vec_count; vec_itr++)
       { 
         ae_int32x2 acc_vec0 = bias;
+        ae_int32x2 acc_vec1 = bias;
         
         AE_LAV8X8X2_XP(vec0_0, vec0_1, align_p_vec_0, p_vec_0, cols1);
         
-        MAT_VEC_MAC(acc_vec0 , acc_vec0 , mat1_row3_0 , mat1_row3_0 , mat1_row3_0 , mat1_row3_0 ,vec0_0, -mat1_zero_bias, -vec1_zero_bias);
-        MAT_VEC_MAC(acc_vec0 , acc_vec0 , mat1_row3_1 , mat1_row3_1 , mat1_row3_1 , mat1_row3_1 ,vec0_1, -mat1_zero_bias, -vec1_zero_bias);
+        MAT_VEC_MAC(acc_vec0 , acc_vec1 , mat1_row3_0 , mat1_row3_0 , mat1_row3_0 , mat1_row3_0 ,vec0_0, -mat1_zero_bias, -vec1_zero_bias);
+        MAT_VEC_MAC(acc_vec0 , acc_vec1 , mat1_row3_1 , mat1_row3_1 , mat1_row3_1 , mat1_row3_1 ,vec0_1, -mat1_zero_bias, -vec1_zero_bias);
        
         ae_int16x4 out_0;
         
-        MPY_BY_QUANT_MULT_X2X2_OUT16_ZB(out_0, acc_vec0, acc_vec0, out_multiplier, left_shift, right_shift, out_zero_bias);
+        MPY_BY_QUANT_MULT_X2X2_OUT16_ZB(out_0, acc_vec0, acc_vec1, out_multiplier, left_shift, right_shift, out_zero_bias);
 
         AE_MINMAX16(out_0, AE_MOVDA16(-128), AE_MOVDA16(127));        
         ae_int8x8 temp_vec0 = AE_SAT8X8X16(out_0, out_0);
@@ -2541,7 +2542,7 @@ WORD32 xa_nn_matmul_asym8sxasym8s_asym8s(
 
         ae_int16x4 out_0;
 
-        MPY_BY_QUANT_MULT_X2X2_OUT16_ZB(out_0, acc_row0_vec0, acc_row0_vec0, out_multiplier, left_shift, right_shift, out_zero_bias);
+        MPY_BY_QUANT_MULT_X2X2_OUT16_ZB(out_0, acc_row0_vec0, acc_row0_vec1, out_multiplier, left_shift, right_shift, out_zero_bias);
         AE_MINMAX16(out_0, AE_MOVDA16(-128), AE_MOVDA16(127));
 
         ae_int8x8 temp_vec0;
diff --git a/algo/kernels/basic/hifi5/xa_nn_elm_add_quant8.c b/algo/kernels/basic/hifi5/xa_nn_elm_add_quant8.c
index 1eb2d89..eef1d6b 100644
--- a/algo/kernels/basic/hifi5/xa_nn_elm_add_quant8.c
+++ b/algo/kernels/basic/hifi5/xa_nn_elm_add_quant8.c
@@ -734,6 +734,48 @@ WORD32 xa_nn_elm_add_broadcast_4D_asym8sxasym8s_asym8s(WORD8 * __restrict__ p_ou
                 1,
                 p_out_shape[0] * inp1_strides[0]);
   }
+  else if(inp1_const == 1 || inp2_const == 1)
+  {
+    WORD32 inp1_zb, inp1_ls, inp1_mult;
+    WORD32 inp2_zb, inp2_ls, inp2_mult;
+    inp1_zb = inp1_zero_bias;
+    inp1_ls = inp1_left_shift;
+    inp1_mult = inp1_multiplier;
+    inp2_zb = inp2_zero_bias;
+    inp2_ls = inp2_left_shift;
+    inp2_mult = inp2_multiplier;
+    if(inp1_const == 1)
+    {
+      inp2_zb = inp1_zero_bias;
+      inp2_ls = inp1_left_shift;
+      inp2_mult = inp1_multiplier;
+      inp1_zb = inp2_zero_bias;
+      inp1_ls = inp2_left_shift;
+      inp1_mult = inp2_multiplier;
+
+      const WORD8 *tmp;
+      tmp = p_inp1_tmp;   p_inp1_tmp = p_inp2_tmp;    p_inp2_tmp = tmp;
+    }
+    {
+      internal_elm_add_broadcast_asym8sxasym8s_asym8s(
+          p_out_tmp,
+          out_zero_bias,
+          out_left_shift,
+          out_multiplier,
+          out_activation_min,
+          out_activation_max,
+          p_inp1_tmp,
+          inp1_zb,
+          inp1_ls,
+          inp1_mult,
+          p_inp2_tmp,
+          inp2_zb,
+          inp2_ls,
+          inp2_mult,
+          left_shift,
+          p_out_shape[0] * p_out_shape[1] * p_out_shape[2] * p_out_shape[3]);
+    }
+  }
   else if(inp1_strides[3] == inp2_strides[3])
   {
     WORD32 in_lc, out_lc;
@@ -810,48 +852,6 @@ WORD32 xa_nn_elm_add_broadcast_4D_asym8sxasym8s_asym8s(WORD8 * __restrict__ p_ou
       p_inp2_tmp += inp2_strides[0];
     }
   }
-  else if(inp1_const == 1 || inp2_const == 1)
-  {
-    WORD32 inp1_zb, inp1_ls, inp1_mult;
-    WORD32 inp2_zb, inp2_ls, inp2_mult;
-    inp1_zb = inp1_zero_bias;
-    inp1_ls = inp1_left_shift;
-    inp1_mult = inp1_multiplier;
-    inp2_zb = inp2_zero_bias;
-    inp2_ls = inp2_left_shift;
-    inp2_mult = inp2_multiplier;
-    if(inp1_const == 1)
-    {
-      inp2_zb = inp1_zero_bias;
-      inp2_ls = inp1_left_shift;
-      inp2_mult = inp1_multiplier;
-      inp1_zb = inp2_zero_bias;
-      inp1_ls = inp2_left_shift;
-      inp1_mult = inp2_multiplier;
-
-      const WORD8 *tmp;
-      tmp = p_inp1_tmp;   p_inp1_tmp = p_inp2_tmp;    p_inp2_tmp = tmp;
-    }
-    {
-      internal_elm_add_broadcast_asym8sxasym8s_asym8s(
-          p_out_tmp,
-          out_zero_bias,
-          out_left_shift,
-          out_multiplier,
-          out_activation_min,
-          out_activation_max,
-          p_inp1_tmp,
-          inp1_zb,
-          inp1_ls,
-          inp1_mult,
-          p_inp2_tmp,
-          inp2_zb,
-          inp2_ls,
-          inp2_mult,
-          left_shift,
-          p_out_shape[0] * p_out_shape[1] * p_out_shape[2] * p_out_shape[3]);
-    }
-  }
   else
   {
     WORD32 inp1_zb, inp1_ls, inp1_mult;
diff --git a/algo/kernels/basic/hifi5/xa_nn_elm_mul_quant8.c b/algo/kernels/basic/hifi5/xa_nn_elm_mul_quant8.c
index 94f3dcf..8ea1f2c 100644
--- a/algo/kernels/basic/hifi5/xa_nn_elm_mul_quant8.c
+++ b/algo/kernels/basic/hifi5/xa_nn_elm_mul_quant8.c
@@ -601,6 +601,33 @@ WORD32 xa_nn_elm_mul_broadcast_4D_asym8sxasym8s_asym8s(WORD8 * __restrict__ p_ou
                 1,
                 p_out_shape[0] * inp1_strides[0]);
   }
+  else if(inp1_const == 1 || inp2_const == 1)
+  {
+    WORD32 inp1_zb;
+    WORD32 inp2_zb;
+    inp1_zb = inp1_zero_bias;
+    inp2_zb = inp2_zero_bias;
+    if(inp1_const == 1)
+    {
+      inp2_zb = inp1_zero_bias;
+      inp1_zb = inp2_zero_bias;
+      const WORD8 *tmp;
+      tmp = p_inp1_tmp;   p_inp1_tmp = p_inp2_tmp;    p_inp2_tmp = tmp;
+    }
+
+    internal_elm_mul_broadcast_asym8sxasym8s_asym8s(
+        p_out_tmp,
+        out_zero_bias,
+        out_shift,
+        out_multiplier,
+        out_activation_min,
+        out_activation_max,
+        p_inp1_tmp,
+        inp1_zb,
+        p_inp2_tmp,
+        inp2_zb,
+        p_out_shape[0] * p_out_shape[1] * p_out_shape[2] * p_out_shape[3]);
+  }
   else if(inp1_strides[3] == inp2_strides[3])
   {
     WORD32 in_lc, out_lc;
@@ -664,33 +691,6 @@ WORD32 xa_nn_elm_mul_broadcast_4D_asym8sxasym8s_asym8s(WORD8 * __restrict__ p_ou
       p_inp2_tmp += inp2_strides[0];
     }
   }
-  else if(inp1_const == 1 || inp2_const == 1)
-  {
-    WORD32 inp1_zb;
-    WORD32 inp2_zb;
-    inp1_zb = inp1_zero_bias;
-    inp2_zb = inp2_zero_bias;
-    if(inp1_strides[3] == 0)
-    {
-      inp2_zb = inp1_zero_bias;
-      inp1_zb = inp2_zero_bias;
-      const WORD8 *tmp;
-      tmp = p_inp1_tmp;   p_inp1_tmp = p_inp2_tmp;    p_inp2_tmp = tmp;
-    }
-
-    internal_elm_mul_broadcast_asym8sxasym8s_asym8s(
-        p_out_tmp,
-        out_zero_bias,
-        out_shift,
-        out_multiplier,
-        out_activation_min,
-        out_activation_max,
-        p_inp1_tmp,
-        inp1_zb,
-        p_inp2_tmp,
-        inp2_zb,
-        p_out_shape[0] * p_out_shape[1] * p_out_shape[2] * p_out_shape[3]);
-  }
   else
   {
     WORD32 inp1_zb;
diff --git a/algo/kernels/basic/hifi5/xa_nn_elm_squared_diff_quant8.c b/algo/kernels/basic/hifi5/xa_nn_elm_squared_diff_quant8.c
index 1ec8a9a..22913b6 100644
--- a/algo/kernels/basic/hifi5/xa_nn_elm_squared_diff_quant8.c
+++ b/algo/kernels/basic/hifi5/xa_nn_elm_squared_diff_quant8.c
@@ -458,6 +458,46 @@ WORD32 xa_nn_elm_squared_diff_broadcast_4D_asym8sxasym8s_asym8s(WORD8 * __restri
                 1,
                 p_out_shape[0] * inp1_strides[0]);
   }
+  else if(inp1_const == 1 || inp2_const == 1)
+  {
+    WORD32 inp1_zb, inp1_ls, inp1_mult;
+    WORD32 inp2_zb, inp2_ls, inp2_mult;
+    inp1_zb = inp1_zero_bias;
+    inp1_ls = inp1_left_shift;
+    inp1_mult = inp1_multiplier;
+    inp2_zb = inp2_zero_bias;
+    inp2_ls = inp2_left_shift;
+    inp2_mult = inp2_multiplier;
+    /* Reversing the inputs is okay because difference is squared */
+    if(inp1_const == 1)
+    {
+      inp2_zb = inp1_zero_bias;
+      inp2_ls = inp1_left_shift;
+      inp2_mult = inp1_multiplier;
+      inp1_zb = inp2_zero_bias;
+      inp1_ls = inp2_left_shift;
+      inp1_mult = inp2_multiplier;
+      const WORD8 *tmp;
+      tmp = p_inp1_tmp;   p_inp1_tmp = p_inp2_tmp;    p_inp2_tmp = tmp;
+    }
+    internal_elm_squared_diff_broadcast_asym8sxasym8s_asym8s(
+        p_out_tmp,
+        out_zero_bias,
+        out_left_shift,
+        out_multiplier,
+        out_activation_min,
+        out_activation_max,
+        p_inp1_tmp,
+        inp1_zb,
+        inp1_ls,
+        inp1_mult,
+        p_inp2_tmp,
+        inp2_zb,
+        inp2_ls,
+        inp2_mult,
+        left_shift,
+        p_out_shape[0] * p_out_shape[1] * p_out_shape[2] * p_out_shape[3]);
+  }
   else if(inp1_strides[3] == inp2_strides[3])
   {
     WORD32 in_lc, out_lc;
@@ -536,46 +576,6 @@ WORD32 xa_nn_elm_squared_diff_broadcast_4D_asym8sxasym8s_asym8s(WORD8 * __restri
       p_inp2_tmp += inp2_strides[0];
     }
   }
-  else if(inp1_const == 1 || inp2_const == 1)
-  {
-    WORD32 inp1_zb, inp1_ls, inp1_mult;
-    WORD32 inp2_zb, inp2_ls, inp2_mult;
-    inp1_zb = inp1_zero_bias;
-    inp1_ls = inp1_left_shift;
-    inp1_mult = inp1_multiplier;
-    inp2_zb = inp2_zero_bias;
-    inp2_ls = inp2_left_shift;
-    inp2_mult = inp2_multiplier;
-    /* Reversing the inputs is okay because difference is squared */
-    if(inp1_strides[3] == 0)
-    {
-      inp2_zb = inp1_zero_bias;
-      inp2_ls = inp1_left_shift;
-      inp2_mult = inp1_multiplier;
-      inp1_zb = inp2_zero_bias;
-      inp1_ls = inp2_left_shift;
-      inp1_mult = inp2_multiplier;
-      const WORD8 *tmp;
-      tmp = p_inp1_tmp;   p_inp1_tmp = p_inp2_tmp;    p_inp2_tmp = tmp;
-    }
-    internal_elm_squared_diff_broadcast_asym8sxasym8s_asym8s(
-        p_out_tmp,
-        out_zero_bias,
-        out_left_shift,
-        out_multiplier,
-        out_activation_min,
-        out_activation_max,
-        p_inp1_tmp,
-        inp1_zb,
-        inp1_ls,
-        inp1_mult,
-        p_inp2_tmp,
-        inp2_zb,
-        inp2_ls,
-        inp2_mult,
-        left_shift,
-        p_out_shape[0] * p_out_shape[1] * p_out_shape[2] * p_out_shape[3]);
-  }
   else
   {
     WORD32 inp1_zb, inp1_ls, inp1_mult;
diff --git a/algo/kernels/activations/hifi5/xa_nn_activations_asym16_asym16.c b/algo/kernels/activations/hifi5/xa_nn_activations_asym16_asym16.c
index b066d41..55c4750 100644
--- a/algo/kernels/activations/hifi5/xa_nn_activations_asym16_asym16.c
+++ b/algo/kernels/activations/hifi5/xa_nn_activations_asym16_asym16.c
@@ -46,7 +46,6 @@ WORD32 xa_nn_vec_leaky_relu_asym16s_asym16s( WORD16 * __restrict__ p_out,
   XA_NNLIB_ARG_CHK_COND(((inp_zero_bias < -32768) || (inp_zero_bias > 32767)), -1);
   XA_NNLIB_ARG_CHK_COND(((out_shift < -31) || (out_shift > 31)), -1);
   XA_NNLIB_ARG_CHK_COND(((alpha_shift < -31) || (alpha_shift > 31)), -1);
-  XA_NNLIB_ARG_CHK_COND((alpha_multiplier < 0), -1);
   XA_NNLIB_ARG_CHK_COND((out_multiplier < 0), -1);
   XA_NNLIB_ARG_CHK_COND(((out_zero_bias < -32768) || (out_zero_bias > 32767)), -1);
 
diff --git a/algo/kernels/activations/hifi5/xa_nn_activations_asym8_asym8.c b/algo/kernels/activations/hifi5/xa_nn_activations_asym8_asym8.c
index f5e5e71..337c46e 100644
--- a/algo/kernels/activations/hifi5/xa_nn_activations_asym8_asym8.c
+++ b/algo/kernels/activations/hifi5/xa_nn_activations_asym8_asym8.c
@@ -1678,7 +1678,6 @@ WORD32 xa_nn_vec_leaky_relu_asym8s_asym8s( WORD8 * __restrict__ p_out,
   XA_NNLIB_ARG_CHK_COND(((inp_zero_bias < -128) || (inp_zero_bias > 127)), -1);
   XA_NNLIB_ARG_CHK_COND(((out_shift < -31) || (out_shift > 31)), -1);
   XA_NNLIB_ARG_CHK_COND(((alpha_shift < -31) || (alpha_shift > 31)), -1);
-  XA_NNLIB_ARG_CHK_COND((alpha_multiplier < 0), -1);
   XA_NNLIB_ARG_CHK_COND((out_multiplier < 0), -1);
   XA_NNLIB_ARG_CHK_COND(((out_zero_bias < -128) || (out_zero_bias > 127)), -1);
 
diff --git a/algo/kernels/cnn/hifi5/xa_nn_matXvec_f32_circ.c b/algo/kernels/cnn/hifi5/xa_nn_matXvec_f32_circ.c
index 943fb45..d429ecd 100644
--- a/algo/kernels/cnn/hifi5/xa_nn_matXvec_f32_circ.c
+++ b/algo/kernels/cnn/hifi5/xa_nn_matXvec_f32_circ.c
@@ -85,8 +85,16 @@ DISCARD_FUN_FOR_NONVOID_RETURN(WORD32, xa_nn_matXvec_f32_circ,(
 #define KERNEL_MAT_VEC_BATCH_ROW_f32(idx_row)\
     KERNEL_MAT_VEC_BATCH_VEC_UNROLL(idx_row);\
 
+#if NO_AGGR_FLOAT_OPT
 #define KERNEL_MAT_VEC_BATCH_f32(idx_row,idx_vec) \
-    MADD_SX2X2(_xtfloatx2_acc_ ##idx_row ##_ ##idx_vec,_xtfloatx2_acc_1_ ##idx_row ##_ ##idx_vec, _xtfloatx2_vec_batch_ ##idx_vec, _xtfloatx2_vec_batch_1_ ##idx_vec, _xtfloatx2_mat_ ##idx_row,_xtfloatx2_mat_1_ ##idx_row);\
+    xtfloatx2 _xtfloatx2_temp0_ ##idx_row ##_ ##idx_vec; \
+    xtfloatx2 _xtfloatx2_temp1_ ##idx_row ##_ ##idx_vec; \
+    MUL_SX2X2(_xtfloatx2_temp0_ ##idx_row ##_ ##idx_vec,_xtfloatx2_temp1_ ##idx_row ##_ ##idx_vec, _xtfloatx2_vec_batch_ ##idx_vec, _xtfloatx2_vec_batch_1_ ##idx_vec, _xtfloatx2_mat_ ##idx_row,_xtfloatx2_mat_1_ ##idx_row);\
+    ADD_SX2X2(_xtfloatx2_acc_ ##idx_row ##_ ##idx_vec,_xtfloatx2_acc_1_ ##idx_row ##_ ##idx_vec, _xtfloatx2_acc_ ##idx_row ##_ ##idx_vec,_xtfloatx2_acc_1_ ##idx_row ##_ ##idx_vec, _xtfloatx2_temp0_ ##idx_row ##_ ##idx_vec, _xtfloatx2_temp1_ ##idx_row ##_ ##idx_vec);
+#else /* NO_AGGR_FLOAT_OPT */
+#define KERNEL_MAT_VEC_BATCH_f32(idx_row,idx_vec) \
+    MADD_SX2X2(_xtfloatx2_acc_ ##idx_row ##_ ##idx_vec,_xtfloatx2_acc_1_ ##idx_row ##_ ##idx_vec, _xtfloatx2_vec_batch_ ##idx_vec, _xtfloatx2_vec_batch_1_ ##idx_vec, _xtfloatx2_mat_ ##idx_row,_xtfloatx2_mat_1_ ##idx_row);
+#endif /* NO_AGGR_FLOAT_OPT */
 
 #define ADD_BIAS_BATCH_ROW_ACC_FOR_f32(idx_row)\
     ADD_BIAS_BATCH_ACC_VEC_UNROLL(idx_row);
@@ -165,6 +173,284 @@ DISCARD_FUN_FOR_NONVOID_RETURN(WORD32, xa_nn_matXvec_f32_circ,(
 
 #endif /* (ROW_UNROLL == 4 && VEC_UNROLL == 2)*/
 
+#if NO_AGGR_FLOAT_OPT
+WORD32 xa_nn_matXvec_f32_circ(
+    FLOAT32 *__restrict__ p_out,            /* output pointer */
+    FLOAT32 *__restrict__ p_mat,            /* matrix: rows x cols */
+    FLOAT32 *__restrict__ p_vec,            /* vec: cols x 1 */
+    FLOAT32 *__restrict__ pt_bias,           /* bias TBD: Need array? */
+    WORD32 rows,                            /* Number of rows in matrix */
+    WORD32 cols,                            /* Number of columns in matrix */
+    WORD32 row_offset,                      /* row stride for matrix */
+    WORD32 vec_count,                       /* number of vectors: 2, 4, 2n */
+    WORD32 vec_offset,                      /* offset from current to next vector */
+    WORD32 out_col_offset,
+    WORD32 out_row_offset)
+{
+    xtfloat *p_bias = (xtfloat *)pt_bias;
+    /* Iterators used in for loops */
+    int m_itr, c_itr, vec_itr;
+    xtfloat* p_out_tmp;
+    /* Assign initial value so this value will be used in trailing loop */
+    m_itr = 0;
+    vec_itr = 0;
+    
+    {
+        for (vec_itr = 0; vec_itr < (vec_count & (~0x3)); vec_itr+=4)
+        {
+            m_itr = 0;
+            for(; m_itr < (rows & (~0x1)); m_itr+=2)
+            {
+                xtfloat _xtfloat_bias_0 = ZERO_S();
+                xtfloat _xtfloat_bias_1 = ZERO_S();
+                xtfloat _xtfloat_bias_2 = ZERO_S();
+                xtfloat _xtfloat_bias_3 = ZERO_S();
+                xtfloatx2 _xtfloatx2_bias_01, _xtfloatx2_bias_23;
+                xtfloatx2 acc_row0_vec01 = ZERO_SX2(), acc_row0_vec23 = ZERO_SX2();
+                xtfloatx2 acc_row1_vec01 = ZERO_SX2(), acc_row1_vec23 = ZERO_SX2();
+                xtfloatx2 x00, x11;
+                xtfloatx2 vec01;
+                xtfloatx2 vec23;
+                xtfloatx2 _xtfloatx2_temp_00, _xtfloatx2_temp_01;
+                xtfloatx2 _xtfloatx2_temp_10, _xtfloatx2_temp_11;
+                if(p_bias != NULL){
+                    _xtfloat_bias_0 = p_bias[vec_itr];
+                    _xtfloat_bias_1 = p_bias[vec_itr+1];
+                    _xtfloat_bias_2 = p_bias[vec_itr+2];
+                    _xtfloat_bias_3 = p_bias[vec_itr+3];
+                    _xtfloatx2_bias_01 = XT_SEL32_LL_SX2(AE_MOVXTFLOATX2_FROMXTFLOAT(_xtfloat_bias_0), AE_MOVXTFLOATX2_FROMXTFLOAT(_xtfloat_bias_1));
+                    _xtfloatx2_bias_23 = XT_SEL32_LL_SX2(AE_MOVXTFLOATX2_FROMXTFLOAT(_xtfloat_bias_2), AE_MOVXTFLOATX2_FROMXTFLOAT(_xtfloat_bias_3));
+                }
+ 
+                ae_int64 *p64_x0 = (ae_int64 *) p_mat;
+                ae_int64 *p64_x1 = (ae_int64 *) p_mat;
+                xtfloat *p_vec0  = (xtfloat *)(&p_vec[(vec_itr)*vec_offset]);
+                xtfloat *p_vec1  = (xtfloat *)(&p_vec[(vec_itr+1)*vec_offset]);
+                xtfloat *p_vec2  = (xtfloat *)(&p_vec[(vec_itr+2)*vec_offset]);
+                xtfloat *p_vec3  = (xtfloat *)(&p_vec[(vec_itr+3)*vec_offset]);
+ 
+                xtfloat _xtfloat_mat_0 = ZERO_S();
+                xtfloat _xtfloat_mat_1 = ZERO_S();
+                AE_ADDCIRC_XC(p64_x0, (m_itr)*row_offset*sizeof(FLOAT32));
+                AE_ADDCIRC_XC(p64_x1, (m_itr+1)*row_offset*sizeof(FLOAT32));
+                xtfloat *px0 = (xtfloat*)p64_x0;
+                xtfloat *px1 = (xtfloat*)p64_x1;
+                int k;
+
+                for(k = 0; k < cols; k++, p_vec0++, p_vec1++, p_vec2++, p_vec3++)
+                {
+                    vec01 = XT_SEL32_LL_SX2(AE_MOVXTFLOATX2_FROMXTFLOAT(*(p_vec0)), AE_MOVXTFLOATX2_FROMXTFLOAT(*(p_vec1)));
+                    vec23 = XT_SEL32_LL_SX2(AE_MOVXTFLOATX2_FROMXTFLOAT(*(p_vec2)), AE_MOVXTFLOATX2_FROMXTFLOAT(*(p_vec3)));
+                    
+                    AE_LSXC(_xtfloat_mat_0, px0, sizeof(xtfloat));
+                    AE_LSXC(_xtfloat_mat_1, px1, sizeof(xtfloat));
+                    x00 = AE_MOVXTFLOATX2_FROMXTFLOAT(_xtfloat_mat_0);
+                    x11 = AE_MOVXTFLOATX2_FROMXTFLOAT(_xtfloat_mat_1);
+                    
+                    MUL_SX2X2(_xtfloatx2_temp_00, _xtfloatx2_temp_01, x00, x00, vec01, vec23);
+                    MUL_SX2X2(_xtfloatx2_temp_10, _xtfloatx2_temp_11, x11, x11, vec01, vec23);
+                    ADD_SX2X2(acc_row0_vec01, acc_row0_vec23, acc_row0_vec01, acc_row0_vec23, _xtfloatx2_temp_00, _xtfloatx2_temp_01);
+                    ADD_SX2X2(acc_row1_vec01, acc_row1_vec23, acc_row1_vec01, acc_row1_vec23, _xtfloatx2_temp_10, _xtfloatx2_temp_11);
+                }
+                ADD_SX2X2(acc_row0_vec01, acc_row0_vec23, acc_row0_vec01, acc_row0_vec23, _xtfloatx2_bias_01, _xtfloatx2_bias_23);
+                ADD_SX2X2(acc_row1_vec01, acc_row1_vec23, acc_row1_vec01, acc_row1_vec23, _xtfloatx2_bias_01, _xtfloatx2_bias_23);
+ 
+                p_out_tmp = (xtfloat*)(&(p_out[(vec_itr)*out_col_offset + (m_itr)*out_row_offset]));
+                AE_SSIP(HIGH_S(acc_row0_vec01),p_out_tmp,0);
+                p_out_tmp = (xtfloat*)(&(p_out[(vec_itr+1)*out_col_offset + (m_itr)*out_row_offset]));
+                AE_SSIP(LOW_S(acc_row0_vec01),p_out_tmp,0);
+                p_out_tmp = (xtfloat*)(&(p_out[(vec_itr+2)*out_col_offset + (m_itr)*out_row_offset]));
+                AE_SSIP(HIGH_S(acc_row0_vec23),p_out_tmp,0);
+                p_out_tmp = (xtfloat*)(&(p_out[(vec_itr+3)*out_col_offset + (m_itr)*out_row_offset]));
+                AE_SSIP(LOW_S(acc_row0_vec23),p_out_tmp,0);
+                p_out_tmp = (xtfloat*)(&(p_out[(vec_itr)*out_col_offset + (m_itr+1)*out_row_offset]));
+                AE_SSIP(HIGH_S(acc_row1_vec01),p_out_tmp,0);
+                p_out_tmp = (xtfloat*)(&(p_out[(vec_itr+1)*out_col_offset + (m_itr+1)*out_row_offset]));
+                AE_SSIP(LOW_S(acc_row1_vec01),p_out_tmp,0);
+                p_out_tmp = (xtfloat*)(&(p_out[(vec_itr+2)*out_col_offset + (m_itr+1)*out_row_offset]));
+                AE_SSIP(HIGH_S(acc_row1_vec23),p_out_tmp,0);
+                p_out_tmp = (xtfloat*)(&(p_out[(vec_itr+3)*out_col_offset + (m_itr+1)*out_row_offset]));
+                AE_SSIP(LOW_S(acc_row1_vec23),p_out_tmp,0);
+            }
+            for (; m_itr < rows ; m_itr++)
+            {
+                xtfloat _xtfloat_bias_0 = ZERO_S();
+                xtfloat _xtfloat_bias_1 = ZERO_S();
+                xtfloat _xtfloat_bias_2 = ZERO_S();
+                xtfloat _xtfloat_bias_3 = ZERO_S();
+                xtfloatx2 _xtfloatx2_bias_01, _xtfloatx2_bias_23;
+                xtfloatx2 acc_row0_vec01 = ZERO_SX2(), acc_row0_vec23 = ZERO_SX2();
+                xtfloatx2 x00;
+                xtfloatx2 vec01;
+                xtfloatx2 vec23;
+                xtfloatx2 _xtfloatx2_temp_00, _xtfloatx2_temp_01;
+                if(p_bias != NULL){
+                    _xtfloat_bias_0 = p_bias[vec_itr];
+                    _xtfloat_bias_1 = p_bias[vec_itr+1];
+                    _xtfloat_bias_2 = p_bias[vec_itr+2];
+                    _xtfloat_bias_3 = p_bias[vec_itr+3];
+                    _xtfloatx2_bias_01 = XT_SEL32_LL_SX2(AE_MOVXTFLOATX2_FROMXTFLOAT(_xtfloat_bias_0), AE_MOVXTFLOATX2_FROMXTFLOAT(_xtfloat_bias_1));
+                    _xtfloatx2_bias_23 = XT_SEL32_LL_SX2(AE_MOVXTFLOATX2_FROMXTFLOAT(_xtfloat_bias_2), AE_MOVXTFLOATX2_FROMXTFLOAT(_xtfloat_bias_3));
+                }
+ 
+                ae_int64 *p64_x0 = (ae_int64 *) p_mat;
+                xtfloat *p_vec0  = (xtfloat *)(&p_vec[(vec_itr)*vec_offset]);
+                xtfloat *p_vec1  = (xtfloat *)(&p_vec[(vec_itr+1)*vec_offset]);
+                xtfloat *p_vec2  = (xtfloat *)(&p_vec[(vec_itr+2)*vec_offset]);
+                xtfloat *p_vec3  = (xtfloat *)(&p_vec[(vec_itr+3)*vec_offset]);
+ 
+                xtfloat _xtfloat_mat_0 = ZERO_S();
+                AE_ADDCIRC_XC(p64_x0, (m_itr)*row_offset*sizeof(FLOAT32));
+                xtfloat *px0 = (xtfloat*)p64_x0;
+                int k;
+                for(k = 0; k < cols; k++, p_vec0++, p_vec1++, p_vec2++, p_vec3++)
+                {
+                    vec01 = XT_SEL32_LL_SX2(AE_MOVXTFLOATX2_FROMXTFLOAT(*(p_vec0)), AE_MOVXTFLOATX2_FROMXTFLOAT(*(p_vec1)));
+                    vec23 = XT_SEL32_LL_SX2(AE_MOVXTFLOATX2_FROMXTFLOAT(*(p_vec2)), AE_MOVXTFLOATX2_FROMXTFLOAT(*(p_vec3)));
+                    
+                    AE_LSXC(_xtfloat_mat_0, px0, sizeof(xtfloat));
+                    x00 = AE_MOVXTFLOATX2_FROMXTFLOAT(_xtfloat_mat_0);
+                    
+                    MUL_SX2X2(_xtfloatx2_temp_00, _xtfloatx2_temp_01, x00, x00, vec01, vec23);
+                    ADD_SX2X2(acc_row0_vec01, acc_row0_vec23, acc_row0_vec01, acc_row0_vec23, _xtfloatx2_temp_00, _xtfloatx2_temp_01);
+                }
+                ADD_SX2X2(acc_row0_vec01, acc_row0_vec23, acc_row0_vec01, acc_row0_vec23, _xtfloatx2_bias_01, _xtfloatx2_bias_23);
+ 
+                p_out_tmp = (xtfloat *)(&(p_out[(vec_itr)*out_col_offset + (m_itr)*out_row_offset]));
+                AE_SSIP(HIGH_S(acc_row0_vec01),p_out_tmp,0);
+                p_out_tmp = (xtfloat *)(&(p_out[(vec_itr+1)*out_col_offset + (m_itr)*out_row_offset]));
+                AE_SSIP(LOW_S(acc_row0_vec01),p_out_tmp,0);
+                p_out_tmp = (xtfloat *)(&(p_out[(vec_itr+2)*out_col_offset + (m_itr)*out_row_offset]));
+                AE_SSIP(HIGH_S(acc_row0_vec23),p_out_tmp,0);
+                p_out_tmp = (xtfloat *)(&(p_out[(vec_itr+3)*out_col_offset + (m_itr)*out_row_offset]));
+                AE_SSIP(LOW_S(acc_row0_vec23),p_out_tmp,0);
+            }
+        }
+        for (; vec_itr < (vec_count & (~0x1)); vec_itr+=2)
+        {
+            for(m_itr = 0; m_itr < (rows & (~0x1)); m_itr+=2)
+            {
+                xtfloat _xtfloat_bias_0 = ZERO_S();
+                xtfloat _xtfloat_bias_1 = ZERO_S();
+                xtfloatx2 acc_row0_vec0 = ZERO_SX2(), acc_row0_vec1 = ZERO_SX2();
+                xtfloatx2 x00;
+                xtfloatx2 vec0_0;
+                xtfloatx2 vec1_0;
+                xtfloatx2 _xtfloatx2_temp_00, _xtfloatx2_temp_20;
+                if(p_bias != NULL){
+                _xtfloat_bias_0 = p_bias[vec_itr];
+                _xtfloat_bias_1 = p_bias[vec_itr+1];
+                }
+ 
+                ae_int64 *p64_x0 = (ae_int64 *) p_mat;
+                ae_int64 *p64_x1 = (ae_int64 *) p_mat;
+                xtfloat *p_vec0  = (xtfloat *)(&p_vec[(vec_itr)*vec_offset]);
+                xtfloat *p_vec1  = (xtfloat *)(&p_vec[(vec_itr+1)*vec_offset]);
+ 
+                xtfloat _xtfloat_mat_0 = ZERO_S();
+                xtfloat _xtfloat_mat_1 = ZERO_S();
+                AE_ADDCIRC_XC(p64_x0, (m_itr)*row_offset*sizeof(FLOAT32));
+                AE_ADDCIRC_XC(p64_x1, (m_itr+1)*row_offset*sizeof(FLOAT32));
+                xtfloat *px0 = (xtfloat *)p64_x0;
+                xtfloat *px1 = (xtfloat *)p64_x1;
+                int k;
+
+                for(k = 0; k < cols; k++, p_vec0++, p_vec1++)
+                {
+                    AE_LSXC(_xtfloat_mat_0, px0, sizeof(xtfloat));
+                    AE_LSXC(_xtfloat_mat_1, px1, sizeof(xtfloat));
+                    x00 = XT_SEL32_HH_SX2(AE_MOVXTFLOATX2_FROMXTFLOAT(_xtfloat_mat_0), AE_MOVXTFLOATX2_FROMXTFLOAT(_xtfloat_mat_1));
+                    vec0_0 = AE_MOVXTFLOATX2_FROMXTFLOAT(*(p_vec0));
+                    vec1_0 = AE_MOVXTFLOATX2_FROMXTFLOAT(*(p_vec1));
+                   
+                    MUL_SX2X2(_xtfloatx2_temp_00, _xtfloatx2_temp_20, x00, x00, vec0_0, vec1_0);
+                    ADD_SX2X2(acc_row0_vec0, acc_row0_vec1, acc_row0_vec0, acc_row0_vec1, _xtfloatx2_temp_00, _xtfloatx2_temp_20);
+                }
+                ADD_SX2X2(acc_row0_vec0, acc_row0_vec1, acc_row0_vec0, acc_row0_vec1, AE_MOVXTFLOATX2_FROMXTFLOAT(_xtfloat_bias_0), AE_MOVXTFLOATX2_FROMXTFLOAT(_xtfloat_bias_1));
+ 
+                p_out_tmp = (xtfloat *)(&(p_out[(vec_itr)*out_col_offset + (m_itr)*out_row_offset]));
+                AE_SSIP(HIGH_S(acc_row0_vec0),p_out_tmp,0);
+                p_out_tmp = (xtfloat *)(&(p_out[(vec_itr)*out_col_offset + (m_itr+1)*out_row_offset]));
+                AE_SSIP(LOW_S(acc_row0_vec0),p_out_tmp,0);
+                p_out_tmp = (xtfloat *)(&(p_out[(vec_itr+1)*out_col_offset + (m_itr)*out_row_offset]));
+                AE_SSIP(HIGH_S(acc_row0_vec1),p_out_tmp,0);
+                p_out_tmp = (xtfloat *)(&(p_out[(vec_itr+1)*out_col_offset + (m_itr+1)*out_row_offset]));
+                AE_SSIP(LOW_S(acc_row0_vec1),p_out_tmp,0);
+            }
+            for (; m_itr < rows ; m_itr++)
+            {
+                xtfloat _xtfloat_bias_0 = ZERO_S();
+                xtfloat _xtfloat_bias_1 = ZERO_S();
+                xtfloatx2 acc_row0_vec0 = ZERO_SX2(), acc_row0_vec1 = ZERO_SX2();
+                xtfloatx2 x00;
+                xtfloatx2 vec0_0;
+                xtfloatx2 vec1_0;
+                xtfloatx2 _xtfloatx2_temp_00, _xtfloatx2_temp_20;
+                if(p_bias != NULL){
+                _xtfloat_bias_0 = p_bias[vec_itr];
+                _xtfloat_bias_1 = p_bias[vec_itr+1];
+                }
+ 
+                ae_int64 *p64_x0 = (ae_int64 *) p_mat;
+                xtfloat *p_vec0  = (xtfloat *)(&p_vec[(vec_itr)*vec_offset]);
+                xtfloat *p_vec1  = (xtfloat *)(&p_vec[(vec_itr+1)*vec_offset]);
+ 
+                xtfloat _xtfloat_mat_0 = ZERO_S();
+                AE_ADDCIRC_XC(p64_x0, (m_itr)*row_offset*sizeof(FLOAT32));
+                xtfloat *px0 = (xtfloat*)p64_x0;
+                int k;
+                for(k = 0; k < cols; k++, p_vec0++, p_vec1++)
+                {
+                    AE_LSXC(_xtfloat_mat_0, px0, sizeof(xtfloat));
+                    x00 = AE_MOVXTFLOATX2_FROMXTFLOAT(_xtfloat_mat_0);
+                    vec0_0 = AE_MOVXTFLOATX2_FROMXTFLOAT(*(p_vec0));
+                    vec1_0 = AE_MOVXTFLOATX2_FROMXTFLOAT(*(p_vec1));
+                   
+                    MUL_SX2X2(_xtfloatx2_temp_00, _xtfloatx2_temp_20, x00, x00, vec0_0, vec1_0);
+                    ADD_SX2X2(acc_row0_vec0, acc_row0_vec1, acc_row0_vec0, acc_row0_vec1, _xtfloatx2_temp_00, _xtfloatx2_temp_20);
+                }
+                ADD_SX2X2(acc_row0_vec0, acc_row0_vec1, acc_row0_vec0, acc_row0_vec1, AE_MOVXTFLOATX2_FROMXTFLOAT(_xtfloat_bias_0), AE_MOVXTFLOATX2_FROMXTFLOAT(_xtfloat_bias_1));
+ 
+                p_out_tmp = (xtfloat *)(&(p_out[(vec_itr)*out_col_offset + (m_itr)*out_row_offset]));
+                AE_SSIP(HIGH_S(acc_row0_vec0),p_out_tmp,0);
+                p_out_tmp = (xtfloat *)(&(p_out[(vec_itr+1)*out_col_offset + (m_itr)*out_row_offset]));
+                AE_SSIP(HIGH_S(acc_row0_vec1),p_out_tmp,0);
+            }
+        }
+        if(vec_count & 0x1)
+        {
+            vec_itr = vec_count -1;
+            for(m_itr = 0; m_itr < (rows); m_itr ++)
+            {
+                xtfloat _xtfloat_bias = ZERO_S();
+                if(p_bias != NULL){
+                _xtfloat_bias = p_bias[vec_itr];
+                }
+                xtfloat _xtfloat_temp_0_0 = ZERO_S();
+                xtfloat _xtfloat_acc_0_0 = ZERO_S();
+                xtfloat _xtfloat_vec_batch_0  = ZERO_S() ;
+                xtfloat *_xtfloat_p_vec_batch_0  = (xtfloat *)(&p_vec[(vec_itr)*vec_offset]);
+                xtfloat _xtfloat_mat_0 = ZERO_S();
+                ae_int16x4 *_xt16x4_p_mat_0 = (ae_int16x4 *) p_mat;
+                AE_ADDCIRC16X4_XC(_xt16x4_p_mat_0, (m_itr)*row_offset*sizeof(FLOAT32));
+                xtfloat *_xtfloat_p_mat_0 = (xtfloat*)_xt16x4_p_mat_0;
+                for(c_itr = 0; c_itr < cols; c_itr++)
+                {
+                    AE_LSIP(_xtfloat_vec_batch_0, _xtfloat_p_vec_batch_0, INCREMENT_IN_BYTES_FOR_FLOAT32);
+                    AE_LSXC(_xtfloat_mat_0, _xtfloat_p_mat_0, INCREMENT_IN_BYTES_FOR_FLOAT32);
+                    _xtfloat_temp_0_0 = MUL_S(_xtfloat_vec_batch_0,_xtfloat_mat_0);
+
+                    _xtfloat_acc_0_0 = ADD_S(_xtfloat_acc_0_0,_xtfloat_temp_0_0);
+                }
+                _xtfloat_acc_0_0=ADD_S(_xtfloat_acc_0_0,_xtfloat_bias);
+                p_out_tmp = (xtfloat *)(&(p_out[(vec_itr)*out_col_offset + (m_itr)*out_row_offset]));
+                AE_SSIP(_xtfloat_acc_0_0,p_out_tmp,0);
+            }
+        }
+    }
+    return 0;
+}
+#else /* NO_AGGR_FLOAT_OPT */
 WORD32 xa_nn_matXvec_f32_circ(
     FLOAT32 *__restrict__ p_out,            /* output pointer */
     FLOAT32 *__restrict__ p_mat,            /* matrix: rows x cols */
@@ -505,4 +791,6 @@ WORD32 xa_nn_matXvec_f32_circ(
     }
     return 0;
 }
+#endif /* NO_AGGR_FLOAT_OPT */
+
 #endif /* #if !HAVE_VFPU */
diff --git a/algo/kernels/cnn/hifi5/xa_nn_transpose_conv_f32.c b/algo/kernels/cnn/hifi5/xa_nn_transpose_conv_f32.c
index 5277423..b40c258 100644
--- a/algo/kernels/cnn/hifi5/xa_nn_transpose_conv_f32.c
+++ b/algo/kernels/cnn/hifi5/xa_nn_transpose_conv_f32.c
@@ -25,6 +25,12 @@
 #include "xa_nn_transpose_conv_state.h"
 #include <string.h>
 
+#if NO_AGGR_FLOAT_OPT
+#define _MADD_SX2(accum0, temp0, d0, d1) \
+  temp0 = MUL_SX2(AE_MOVXTFLOATX2_FROMXTFLOAT(d0), AE_MOVXTFLOATX2_FROMXTFLOAT(d1)); \
+  accum0 = AE_MOVXTFLOAT_FROMXTFLOATX2(ADD_SX2(AE_MOVXTFLOATX2_FROMXTFLOAT(accum0), temp0));
+#endif /* NO_AGGR_FLOAT_OPT */
+
 #if !HAVE_VFPU
 DISCARD_FUN_FOR_NONVOID_RETURN(WORD32, xa_nn_transpose_conv_f32, (FLOAT32* output_data,
             const FLOAT32* input_data,
@@ -39,6 +45,325 @@ DISCARD_FUN_FOR_NONVOID_RETURN(WORD32, xa_nn_transpose_conv_f32, (FLOAT32* outpu
             int num_elements, int num_groups,
             void* scratch_buffer))
 #else
+#if NO_AGGR_FLOAT_OPT
+static inline void tconv2d_f32(FLOAT32* output_data,
+    const FLOAT32* input_data,
+    const FLOAT32* filter_data,
+    const FLOAT32* bias_data,
+    int stride_width, int stride_height,
+    int pad_width, int pad_height,
+    int input_depth, int output_depth,
+    int input_height, int input_width,
+    int filter_height, int filter_width,
+    int output_height, int output_width,
+    int num_elements,
+    FLOAT32* scratch_buffer)
+{
+  /* scratch memory is as big as output buffer, and stores 1 element per output */
+  memset(scratch_buffer, 0, num_elements*sizeof(FLOAT32));
+  ae_int32 *pscratch32 = (ae_int32 *)scratch_buffer;
+
+  int stride1 = filter_height*filter_width*input_depth*sizeof(FLOAT32);
+
+  if(input_data && filter_data && output_data && scratch_buffer &&
+    (((unsigned int)input_data&0x7)==0) && (((unsigned int)filter_data&0x7)==0) && (((unsigned int)output_data&0x7) == 0) &&
+    (((unsigned int)scratch_buffer&0xF) == 0) && ((input_depth&0x7)==0) && ((filter_height*filter_width&0x3)==0) && ((output_depth&0x3)==0))
+  {
+    for (int in_y = 0; in_y < input_height; ++in_y)
+    {
+      for (int in_x = 0; in_x < input_width; ++in_x)
+      {
+        const int out_x_orig = in_x*stride_width - pad_width;
+        const int out_y_orig = in_y*stride_height - pad_height;
+        int filt_x_min = -out_x_orig;
+        int filt_x_max = output_width - out_x_orig;
+        int filt_y_min = -out_y_orig;
+        int filt_y_max = output_height - out_y_orig;
+        filt_x_min = (filt_x_min < filter_width) ? filt_x_min : filter_width;
+        filt_x_min = (filt_x_min < 0) ? 0 : filt_x_min;
+        filt_x_max = (filt_x_max < filter_width) ? filt_x_max : filter_width;
+        filt_x_max = (filt_x_max < 0) ? 0 : filt_x_max;
+        filt_y_min = (filt_y_min < filter_height) ? filt_y_min : filter_height;
+        filt_y_min = (filt_y_min < 0) ? 0 : filt_y_min;
+        filt_y_max = (filt_y_max < filter_height) ? filt_y_max : filter_height;
+        filt_y_max = (filt_y_max < 0) ? 0 : filt_y_max;
+        xtfloatx4 * __restrict__ pinp = (xtfloatx4 *)((FLOAT32*)&input_data[in_y*input_width*input_depth+in_x*input_depth]);
+
+        xtfloatx2 *p_inp_align = (xtfloatx2 *)pinp;
+      
+        int in_channel;
+
+        xtfloatx2 _xtfloatx2_temp_00 = ZERO_SX2();
+        xtfloatx2 _xtfloatx2_temp_01 = ZERO_SX2();
+        xtfloatx2 _xtfloatx2_temp_02 = ZERO_SX2();
+        xtfloatx2 _xtfloatx2_temp_03 = ZERO_SX2();
+        for (in_channel=0 ; in_channel < (input_depth & ~0x3); in_channel+=4)
+        {
+          xtfloatx2 d_inp, d_inp1;
+          AE_LSX2X2_IP(d_inp, d_inp1, pinp, 4*sizeof(FLOAT32));
+          for (int filter_y = filt_y_min; filter_y < filt_y_max; ++filter_y)
+          {
+            for (int filter_x = filt_x_min; filter_x < filt_x_max; ++filter_x)
+            {
+              // Compute output element location.
+              int out_x = out_x_orig + filter_x;
+              int out_y = out_y_orig + filter_y;
+              xtfloatx4* __restrict__ pscratch_src = (xtfloatx4 *)((FLOAT32*)&pscratch32[out_y*output_width*output_depth+out_x*output_depth]);
+              xtfloatx4* __restrict__ pfilt = (xtfloatx4 *)((FLOAT32*)&filter_data[filter_y*filter_width*input_depth + filter_x*input_depth + in_channel]);
+
+              int out_channel = 0;
+
+              for (; out_channel < (output_depth&~0x7); out_channel+=8)
+              {
+                xtfloatx2 d_fil0, d_fil10;
+                xtfloatx2 d_fil1, d_fil11;
+                xtfloatx2 d_fil2, d_fil12;
+                xtfloatx2 d_fil3, d_fil13;
+                xtfloatx2 d_fil4, d_fil14;
+                xtfloatx2 d_fil5, d_fil15;
+                xtfloatx2 d_fil6, d_fil16;
+                xtfloatx2 d_fil7, d_fil17;
+                xtfloatx2 d_scr0;
+                xtfloatx2 d_scr1;
+                xtfloatx2 d_scr2;
+                xtfloatx2 d_scr3;
+                AE_LSX2X2_I(d_scr0, d_scr1, pscratch_src, 0);
+                AE_LSX2X2_I(d_scr2, d_scr3, pscratch_src, 16);
+                AE_LSX2X2_XP(d_fil0, d_fil10, pfilt, stride1);
+                AE_LSX2X2_XP(d_fil1, d_fil11, pfilt, stride1);
+                AE_LSX2X2_XP(d_fil2, d_fil12, pfilt, stride1);
+                AE_LSX2X2_XP(d_fil3, d_fil13, pfilt, stride1);
+                AE_LSX2X2_XP(d_fil4, d_fil14, pfilt, stride1);
+                AE_LSX2X2_XP(d_fil5, d_fil15, pfilt, stride1);
+                AE_LSX2X2_XP(d_fil6, d_fil16, pfilt, stride1);
+                AE_LSX2X2_XP(d_fil7, d_fil17, pfilt, stride1);
+
+                MUL_SX2X2(_xtfloatx2_temp_00, _xtfloatx2_temp_01, AE_MOVXTFLOATX2_FROMXTFLOAT(HIGH_S(d_inp)), AE_MOVXTFLOATX2_FROMXTFLOAT(HIGH_S(d_inp)), AE_SEL32_HH_SX2(d_fil0, d_fil1), AE_SEL32_HH_SX2(d_fil2, d_fil3));
+                MUL_SX2X2(_xtfloatx2_temp_02, _xtfloatx2_temp_03, AE_MOVXTFLOATX2_FROMXTFLOAT(HIGH_S(d_inp)), AE_MOVXTFLOATX2_FROMXTFLOAT(HIGH_S(d_inp)), AE_SEL32_HH_SX2(d_fil4, d_fil5), AE_SEL32_HH_SX2(d_fil6, d_fil7));
+                ADD_SX2X2(d_scr0, d_scr1, d_scr0, d_scr1, _xtfloatx2_temp_00, _xtfloatx2_temp_01);
+                ADD_SX2X2(d_scr2, d_scr3, d_scr2, d_scr3, _xtfloatx2_temp_02, _xtfloatx2_temp_03);
+
+                MUL_SX2X2(_xtfloatx2_temp_00, _xtfloatx2_temp_01, AE_MOVXTFLOATX2_FROMXTFLOAT(LOW_S(d_inp)), AE_MOVXTFLOATX2_FROMXTFLOAT(LOW_S(d_inp)), AE_SEL32_LL_SX2(d_fil0, d_fil1), AE_SEL32_LL_SX2(d_fil2, d_fil3));
+                MUL_SX2X2(_xtfloatx2_temp_02, _xtfloatx2_temp_03, AE_MOVXTFLOATX2_FROMXTFLOAT(LOW_S(d_inp)), AE_MOVXTFLOATX2_FROMXTFLOAT(LOW_S(d_inp)), AE_SEL32_LL_SX2(d_fil4, d_fil5), AE_SEL32_LL_SX2(d_fil6, d_fil7));
+                ADD_SX2X2(d_scr0, d_scr1, d_scr0, d_scr1, _xtfloatx2_temp_00, _xtfloatx2_temp_01);
+                ADD_SX2X2(d_scr2, d_scr3, d_scr2, d_scr3, _xtfloatx2_temp_02, _xtfloatx2_temp_03);
+
+                MUL_SX2X2(_xtfloatx2_temp_00, _xtfloatx2_temp_01, AE_MOVXTFLOATX2_FROMXTFLOAT(HIGH_S(d_inp1)), AE_MOVXTFLOATX2_FROMXTFLOAT(HIGH_S(d_inp1)), AE_SEL32_HH_SX2(d_fil10, d_fil11), AE_SEL32_HH_SX2(d_fil12, d_fil13));
+                MUL_SX2X2(_xtfloatx2_temp_02, _xtfloatx2_temp_03, AE_MOVXTFLOATX2_FROMXTFLOAT(HIGH_S(d_inp1)), AE_MOVXTFLOATX2_FROMXTFLOAT(HIGH_S(d_inp1)), AE_SEL32_HH_SX2(d_fil14, d_fil15), AE_SEL32_HH_SX2(d_fil16, d_fil17));
+                ADD_SX2X2(d_scr0, d_scr1, d_scr0, d_scr1, _xtfloatx2_temp_00, _xtfloatx2_temp_01);
+                ADD_SX2X2(d_scr2, d_scr3, d_scr2, d_scr3, _xtfloatx2_temp_02, _xtfloatx2_temp_03);
+
+                MUL_SX2X2(_xtfloatx2_temp_00, _xtfloatx2_temp_01, AE_MOVXTFLOATX2_FROMXTFLOAT(LOW_S(d_inp1)), AE_MOVXTFLOATX2_FROMXTFLOAT(LOW_S(d_inp1)), AE_SEL32_LL_SX2(d_fil10, d_fil11), AE_SEL32_LL_SX2(d_fil12, d_fil13));
+                MUL_SX2X2(_xtfloatx2_temp_02, _xtfloatx2_temp_03, AE_MOVXTFLOATX2_FROMXTFLOAT(LOW_S(d_inp1)), AE_MOVXTFLOATX2_FROMXTFLOAT(LOW_S(d_inp1)), AE_SEL32_LL_SX2(d_fil14, d_fil15), AE_SEL32_LL_SX2(d_fil16, d_fil17));
+                ADD_SX2X2(d_scr0, d_scr1, d_scr0, d_scr1, _xtfloatx2_temp_00, _xtfloatx2_temp_01);
+                ADD_SX2X2(d_scr2, d_scr3, d_scr2, d_scr3, _xtfloatx2_temp_02, _xtfloatx2_temp_03);
+
+                AE_SSX2X2_IP(d_scr0, d_scr1, pscratch_src, 16);
+                AE_SSX2X2_IP(d_scr2, d_scr3, pscratch_src, 16);
+              }
+              xtfloatx2 *pscratchx2_src = (xtfloatx2 *)pscratch_src;
+              for (; out_channel < output_depth; out_channel+=2)
+              {
+                xtfloatx2 d_scr0;
+                xtfloatx2 d_fil0, d_fil10;
+                xtfloatx2 d_fil1, d_fil11;
+                d_scr0 = AE_LSX2I(pscratchx2_src, 0);
+                AE_LSX2X2_XP(d_fil0, d_fil10, pfilt, stride1);
+                AE_LSX2X2_XP(d_fil1, d_fil11, pfilt, stride1);
+
+                _xtfloatx2_temp_00 = MUL_SX2(AE_MOVXTFLOATX2_FROMXTFLOAT(HIGH_S(d_inp)), AE_SEL32_HH_SX2(d_fil0, d_fil1));
+                d_scr0 = ADD_SX2(d_scr0, _xtfloatx2_temp_00);
+                _xtfloatx2_temp_00 = MUL_SX2(AE_MOVXTFLOATX2_FROMXTFLOAT(LOW_S(d_inp)), AE_SEL32_LL_SX2(d_fil0, d_fil1));
+                d_scr0 = ADD_SX2(d_scr0, _xtfloatx2_temp_00);
+                _xtfloatx2_temp_00 = MUL_SX2(AE_MOVXTFLOATX2_FROMXTFLOAT(HIGH_S(d_inp1)), AE_SEL32_HH_SX2(d_fil10, d_fil11));
+                d_scr0 = ADD_SX2(d_scr0, _xtfloatx2_temp_00);
+                _xtfloatx2_temp_00 = MUL_SX2(AE_MOVXTFLOATX2_FROMXTFLOAT(LOW_S(d_inp1)), AE_SEL32_LL_SX2(d_fil10, d_fil11));
+                d_scr0 = ADD_SX2(d_scr0, _xtfloatx2_temp_00);
+
+                AE_SSX2IP(d_scr0, pscratchx2_src, 8);
+              }
+              pscratch_src = (xtfloatx4*)pscratchx2_src;
+            }
+          }
+        }
+        xtfloat *pt_inp_align = (xtfloat *)p_inp_align;
+        for (in_channel=0 ; in_channel < (input_depth & 0x3); in_channel++)
+        {
+          xtfloat d_inp;
+          XT_LSIP(d_inp, pt_inp_align, 4);
+          for (int filter_y = filt_y_min; filter_y < filt_y_max; ++filter_y)
+          {
+            for (int filter_x = filt_x_min; filter_x < filt_x_max; ++filter_x)
+            {
+              // Compute output element location.
+              int out_x = out_x_orig + filter_x;
+              int out_y = out_y_orig + filter_y;
+              xtfloatx4* __restrict__ pscratch_src = (xtfloatx4 *)((FLOAT32*)&pscratch32[out_y*output_width*output_depth+out_x*output_depth]);
+              FLOAT32* __restrict__ pfilt = (FLOAT32*)&filter_data[filter_y*filter_width*input_depth + filter_x*input_depth + in_channel];
+
+              ae_valignx2 align_scr = AE_LA128_PP(pscratch_src);
+              ae_valignx2 align_out;
+              int out_channel = 0;
+
+              for (; out_channel < (output_depth&~0x3); out_channel+=4)
+              {
+                xtfloat   d_fil0;
+                xtfloat   d_fil1;
+                xtfloat   d_fil2;
+                xtfloat   d_fil3;
+                xtfloatx2 d_scr0;
+                xtfloatx2 d_scr1;
+
+                AE_LASX2X2_IP(d_scr0, d_scr1, align_scr, pscratch_src);
+                XT_LSXP(d_fil0, pfilt, stride1);
+                XT_LSXP(d_fil1, pfilt, stride1);
+                XT_LSXP(d_fil2, pfilt, stride1);
+                XT_LSXP(d_fil3, pfilt, stride1);
+
+                MUL_SX2X2(_xtfloatx2_temp_00, _xtfloatx2_temp_01, AE_MOVXTFLOATX2_FROMXTFLOAT(d_inp), AE_MOVXTFLOATX2_FROMXTFLOAT(d_inp), AE_SEL32_LL_SX2(AE_MOVXTFLOATX2_FROMXTFLOAT(d_fil0), AE_MOVXTFLOATX2_FROMXTFLOAT(d_fil1)), AE_SEL32_LL_SX2(AE_MOVXTFLOATX2_FROMXTFLOAT(d_fil2), AE_MOVXTFLOATX2_FROMXTFLOAT(d_fil3)));
+                ADD_SX2X2(d_scr0, d_scr1, d_scr0, d_scr1, _xtfloatx2_temp_00, _xtfloatx2_temp_01);
+                pscratch_src -= 4;
+                AE_SASX2X2_IP(d_scr0, d_scr1, align_out, pscratch_src);
+              }
+              AE_SA128POS_FP(align_out, pscratch_src);
+
+              xtfloat *pscratch_src_t = (xtfloat *)pscratch_src;
+              for (; out_channel < output_depth; out_channel++)
+              {
+                xtfloat   d_fil;
+                xtfloat d_scr0;
+                d_scr0 = XT_LSI(pscratch_src_t, 0);
+                XT_LSXP(d_fil, pfilt, stride1);
+                _MADD_SX2(d_scr0, _xtfloatx2_temp_00, d_inp, d_fil);
+                XT_SSIP(HIGH_S(AE_MOVXTFLOATX2_FROMXTFLOAT(d_scr0)), pscratch_src_t, 4);
+              }
+              pscratch_src = (xtfloatx4*)pscratch_src_t;
+            }
+          }
+        }
+      }
+    }
+  }
+  else
+  {
+    for (int in_y = 0; in_y < input_height; ++in_y)
+    {
+      for (int in_x = 0; in_x < input_width; ++in_x)
+      {
+        const int out_x_orig = in_x*stride_width - pad_width;
+        const int out_y_orig = in_y*stride_height - pad_height;
+        int filt_x_min = -out_x_orig;
+        int filt_x_max = output_width - out_x_orig;
+        int filt_y_min = -out_y_orig;
+        int filt_y_max = output_height - out_y_orig;
+        filt_x_min = (filt_x_min < filter_width) ? filt_x_min : filter_width;
+        filt_x_min = (filt_x_min < 0) ? 0 : filt_x_min;
+        filt_x_max = (filt_x_max < filter_width) ? filt_x_max : filter_width;
+        filt_x_max = (filt_x_max < 0) ? 0 : filt_x_max;
+        filt_y_min = (filt_y_min < filter_height) ? filt_y_min : filter_height;
+        filt_y_min = (filt_y_min < 0) ? 0 : filt_y_min;
+        filt_y_max = (filt_y_max < filter_height) ? filt_y_max : filter_height;
+        filt_y_max = (filt_y_max < 0) ? 0 : filt_y_max;
+        FLOAT32 * __restrict__ pinp =  (FLOAT32*)&input_data[in_y*input_width*input_depth+in_x*input_depth];
+
+        xtfloat *p_inp_align = (xtfloat *)pinp;
+      
+        int in_channel;
+
+        xtfloatx2 _xtfloatx2_temp_00 = ZERO_SX2();
+        xtfloatx2 _xtfloatx2_temp_01 = ZERO_SX2();
+        for (in_channel=0 ; in_channel < (input_depth ); in_channel++)
+        {
+          xtfloat d_inp;
+          XT_LSIP(d_inp, p_inp_align, 4);
+          for (int filter_y = filt_y_min; filter_y < filt_y_max; ++filter_y)
+          {
+            for (int filter_x = filt_x_min; filter_x < filt_x_max; ++filter_x)
+            {
+              // Compute output element location.
+              int out_x = out_x_orig + filter_x;
+              int out_y = out_y_orig + filter_y;
+              xtfloatx4* __restrict__ pscratch_src = (xtfloatx4 *) ((FLOAT32*)&pscratch32[out_y*output_width*output_depth+out_x*output_depth]);
+              FLOAT32* __restrict__ pfilt = (FLOAT32*)&filter_data[filter_y*filter_width*input_depth + filter_x*input_depth + in_channel];
+
+              ae_valignx2 align_scr = AE_LA128_PP(pscratch_src);
+              ae_valignx2 align_out;
+              int out_channel = 0;
+
+              for (; out_channel < (output_depth&~0x3); out_channel+=4)
+              {
+                xtfloat   d_fil0;
+                xtfloat   d_fil1;
+                xtfloat   d_fil2;
+                xtfloat   d_fil3;
+                xtfloatx2 d_scr0;
+                xtfloatx2 d_scr1;
+
+                AE_LASX2X2_IP(d_scr0, d_scr1, align_scr, pscratch_src);
+                XT_LSXP(d_fil0, pfilt, stride1);
+                XT_LSXP(d_fil1, pfilt, stride1);
+                XT_LSXP(d_fil2, pfilt, stride1);
+                XT_LSXP(d_fil3, pfilt, stride1);
+
+                MUL_SX2X2(_xtfloatx2_temp_00, _xtfloatx2_temp_01, AE_MOVXTFLOATX2_FROMXTFLOAT(d_inp), AE_MOVXTFLOATX2_FROMXTFLOAT(d_inp), AE_SEL32_LL_SX2(AE_MOVXTFLOATX2_FROMXTFLOAT(d_fil0), AE_MOVXTFLOATX2_FROMXTFLOAT(d_fil1)), AE_SEL32_LL_SX2(AE_MOVXTFLOATX2_FROMXTFLOAT(d_fil2), AE_MOVXTFLOATX2_FROMXTFLOAT(d_fil3)));
+                ADD_SX2X2(d_scr0, d_scr1, d_scr0, d_scr1, _xtfloatx2_temp_00, _xtfloatx2_temp_01);
+                pscratch_src -= 4;
+                AE_SASX2X2_IP(d_scr0, d_scr1, align_out, pscratch_src);
+              }
+              AE_SA128POS_FP(align_out, pscratch_src);
+
+              xtfloat *pscratch_src_t = (xtfloat *)pscratch_src;
+              for (; out_channel < output_depth; out_channel++)
+              {
+                xtfloat   d_fil;
+                xtfloat d_scr0;
+                d_scr0 = XT_LSI(pscratch_src_t, 0);
+                XT_LSXP(d_fil, pfilt, stride1);
+                _MADD_SX2(d_scr0, _xtfloatx2_temp_00, d_inp, d_fil);
+                XT_SSIP(HIGH_S(AE_MOVXTFLOATX2_FROMXTFLOAT(d_scr0)), pscratch_src_t, 4);
+              }
+              pscratch_src = (xtfloatx4*)pscratch_src_t;
+            }
+          }
+        }
+      }
+    }
+  }
+  if(bias_data)
+  {
+    xtfloat *pbias = (xtfloat*)((FLOAT32*)bias_data);
+
+    for (int out_channel = 0; out_channel < output_depth; ++out_channel)
+    {
+      xtfloat acc;
+      xtfloat dbias;
+      xtfloat *pscratch = (xtfloat *)((FLOAT32*)&pscratch32[out_channel]);
+      FLOAT32 *pout = (FLOAT32*)&output_data[out_channel];
+      XT_LSIP(dbias, pbias, sizeof(FLOAT32));
+
+      for (int i = 0; i < (output_height*output_width); i++)
+      {
+        XT_LSXP(acc, pscratch, output_depth*sizeof(FLOAT32));
+        XT_SSXP(ADD_S(acc, dbias), pout, output_depth*sizeof(FLOAT32));
+      }
+    }
+  }
+  else
+  {
+    xtfloat *pscratch = (xtfloat*)scratch_buffer;
+    FLOAT32 *pout = (FLOAT32*)output_data;
+    for (int i = 0; i < output_height*output_width; i++)
+    {
+      for (int out_channel = 0; out_channel < output_depth; ++out_channel)
+      {
+        xtfloat acc;
+        XT_LSIP(acc, pscratch, sizeof(FLOAT32));
+        XT_SSIP((acc), pout, sizeof(FLOAT32));
+      }
+    }
+  }    
+}
+#else
 static inline void tconv2d_f32(FLOAT32* output_data,
     const FLOAT32* input_data,
     const FLOAT32* filter_data,
@@ -320,6 +645,7 @@ static inline void tconv2d_f32(FLOAT32* output_data,
     }
   }    
 }
+#endif
 
 /* Handle sub-kernel formation and transpose */
 static inline void tconv2d_std_reorder_kernel_f32
diff --git a/algo/kernels/matXvec/hifi5/xa_nn_matmul_f32.c b/algo/kernels/matXvec/hifi5/xa_nn_matmul_f32.c
index 71cb745..e83f797 100644
--- a/algo/kernels/matXvec/hifi5/xa_nn_matmul_f32.c
+++ b/algo/kernels/matXvec/hifi5/xa_nn_matmul_f32.c
@@ -38,6 +38,403 @@ DISCARD_FUN_FOR_NONVOID_RETURN(WORD32,xa_nn_matmul_f32xf32_f32,(
     WORD32 out_stride))                      
 
 #else
+
+#if NO_AGGR_FLOAT_OPT
+/* Using the 4 row 1 vec function defined in xa_nn_matXvec_f32.c for xa_nn_matXvec_f32() kernel */
+static inline void _xa_nn_dot_product_4_rows_1_vecs_offset_aligned
+    (xtfloatx2* out_0_0
+    ,xtfloatx2* out_1_0
+    ,xtfloat*   px0
+    ,xtfloat*   py
+    ,WORD32     cols1
+    ,WORD32     row_stride1
+    )
+{
+  xtfloatx2 acc00, acc20;
+  xtfloatx2 x00, x20;
+  xtfloatx2 y0;
+  xtfloatx2 _xtfloatx2_temp_00, _xtfloatx2_temp_20;
+
+  xtfloat* px1 = px0 + 4*row_stride1; //next 4th row 
+  xtfloat* px2 = px1 + 4*row_stride1; //next 4th row
+  xtfloat* px3 = px2 + 4*row_stride1; //next 4th row 
+  
+  xtfloatx2 z0 = *out_0_0;
+  xtfloatx2 z1 = *out_1_0;
+
+  /* Pre loop computation */
+  int k;
+
+  acc00 = ZERO_SX2();
+  acc20 = ZERO_SX2();
+  for(k = 0; k < cols1; k++, px0++, px1++, px2++, px3++, py++)
+  {
+      x00 = XT_SEL32_LL_SX2(AE_MOVXTFLOATX2_FROMXTFLOAT(*(px0)), AE_MOVXTFLOATX2_FROMXTFLOAT(*(px1)));
+      x20 = XT_SEL32_LL_SX2(AE_MOVXTFLOATX2_FROMXTFLOAT(*(px2)), AE_MOVXTFLOATX2_FROMXTFLOAT(*(px3)));
+      y0 = AE_MOVXTFLOATX2_FROMXTFLOAT(*(py));
+      MUL_SX2X2(_xtfloatx2_temp_00, _xtfloatx2_temp_20, x00, x20, y0, y0);
+      ADD_SX2X2(acc00, acc20, acc00, acc20, _xtfloatx2_temp_00, _xtfloatx2_temp_20);
+
+  }
+  z0 = ADD_SX2(z0, acc00);
+  z1 = ADD_SX2(z1, acc20);
+
+  *out_0_0 = z0;
+  *out_1_0 = z1;
+}
+
+static inline void _xa_nn_dot_product_4_rows_4_vecs_offset_aligned
+    (xtfloatx2* out_0_0
+    ,xtfloatx2* out_1_0
+    ,xtfloatx2* out_0_1
+    ,xtfloatx2* out_1_1
+    ,xtfloatx2* out_0_2
+    ,xtfloatx2* out_1_2
+    ,xtfloatx2* out_0_3
+    ,xtfloatx2* out_1_3
+    ,xtfloat*   px0
+    ,xtfloat*   p_vec0
+    ,WORD32     cols1
+    ,WORD32     row_stride1
+    ,WORD32     vec_offset
+    )
+{
+  xtfloatx2 acc_row0_vec0, acc_row0_vec1;
+  xtfloatx2 acc_row2_vec0, acc_row2_vec1;
+  xtfloatx2 acc_row0_vec2, acc_row0_vec3;
+  xtfloatx2 acc_row2_vec2, acc_row2_vec3;
+  xtfloatx2 x00;
+  xtfloatx2 x20;
+  xtfloatx2 vec0_0;
+  xtfloatx2 vec1_0;
+  xtfloatx2 vec2_0;
+  xtfloatx2 vec3_0;
+ 
+  xtfloat* px1 = px0 + 4*row_stride1; //next 4th row
+  xtfloat* px2 = px1 + 4*row_stride1; //next 4th row
+  xtfloat* px3 = px2 + 4*row_stride1; //next 4th row
+
+  xtfloat *p_vec1  = (xtfloat *)(p_vec0 + vec_offset);
+  xtfloat *p_vec2  = (xtfloat *)(p_vec1 + vec_offset);
+  xtfloat *p_vec3  = (xtfloat *)(p_vec2 + vec_offset);
+  
+  xtfloatx2 z0 = *out_0_0;
+  xtfloatx2 z1 = *out_1_0;
+  xtfloatx2 z2 = *out_0_1;
+  xtfloatx2 z3 = *out_1_1;
+  xtfloatx2 z4 = *out_0_2;
+  xtfloatx2 z5 = *out_1_2;
+  xtfloatx2 z6 = *out_0_3;
+  xtfloatx2 z7 = *out_1_3;
+ 
+  int k;
+  
+  acc_row0_vec0 = acc_row2_vec0 = ZERO_SX2();
+  acc_row0_vec1 = acc_row2_vec1 = ZERO_SX2();
+  acc_row0_vec2 = acc_row2_vec2 = ZERO_SX2();
+  acc_row0_vec3 = acc_row2_vec3 = ZERO_SX2();
+  xtfloatx2 _xtfloatx2_temp_00, _xtfloatx2_temp_20;
+  xtfloatx2 _xtfloatx2_temp_01, _xtfloatx2_temp_21;
+  xtfloatx2 _xtfloatx2_temp_02, _xtfloatx2_temp_22;
+  xtfloatx2 _xtfloatx2_temp_03, _xtfloatx2_temp_23;
+  for(k = 0; k < cols1; k++, px0++, px1++, px2++, px3++, p_vec0++, p_vec1++, p_vec2++, p_vec3++)
+  {
+      x00 = XT_SEL32_LL_SX2(AE_MOVXTFLOATX2_FROMXTFLOAT(*(px0)), AE_MOVXTFLOATX2_FROMXTFLOAT(*(px1)));
+      x20 = XT_SEL32_LL_SX2(AE_MOVXTFLOATX2_FROMXTFLOAT(*(px2)), AE_MOVXTFLOATX2_FROMXTFLOAT(*(px3)));
+      vec0_0 = AE_MOVXTFLOATX2_FROMXTFLOAT(*(p_vec0));
+      vec1_0 = AE_MOVXTFLOATX2_FROMXTFLOAT(*(p_vec1));
+      vec2_0 = AE_MOVXTFLOATX2_FROMXTFLOAT(*(p_vec2));
+      vec3_0 = AE_MOVXTFLOATX2_FROMXTFLOAT(*(p_vec3));
+      
+      MUL_SX2X2(_xtfloatx2_temp_00, _xtfloatx2_temp_20, x00, x20, vec0_0, vec0_0);
+      MUL_SX2X2(_xtfloatx2_temp_01, _xtfloatx2_temp_21, x00, x20, vec1_0, vec1_0);
+      MUL_SX2X2(_xtfloatx2_temp_02, _xtfloatx2_temp_22, x00, x20, vec2_0, vec2_0);
+      MUL_SX2X2(_xtfloatx2_temp_03, _xtfloatx2_temp_23, x00, x20, vec3_0, vec3_0);
+      ADD_SX2X2(acc_row0_vec0, acc_row2_vec0, acc_row0_vec0, acc_row2_vec0, _xtfloatx2_temp_00, _xtfloatx2_temp_20);
+      ADD_SX2X2(acc_row0_vec1, acc_row2_vec1, acc_row0_vec1, acc_row2_vec1, _xtfloatx2_temp_01, _xtfloatx2_temp_21);
+      ADD_SX2X2(acc_row0_vec2, acc_row2_vec2, acc_row0_vec2, acc_row2_vec2, _xtfloatx2_temp_02, _xtfloatx2_temp_22);
+      ADD_SX2X2(acc_row0_vec3, acc_row2_vec3, acc_row0_vec3, acc_row2_vec3, _xtfloatx2_temp_03, _xtfloatx2_temp_23);
+  }
+  z0 = ADD_SX2(z0, acc_row0_vec0);
+  z1 = ADD_SX2(z1, acc_row2_vec0);
+  z2 = ADD_SX2(z2, acc_row0_vec1);
+  z3 = ADD_SX2(z3, acc_row2_vec1);
+  z4 = ADD_SX2(z4, acc_row0_vec2);
+  z5 = ADD_SX2(z5, acc_row2_vec2);
+  z6 = ADD_SX2(z6, acc_row0_vec3);
+  z7 = ADD_SX2(z7, acc_row2_vec3);
+
+  *out_0_0 = z0;
+  *out_1_0 = z1;
+  *out_0_1 = z2;
+  *out_1_1 = z3;
+  *out_0_2 = z4;
+  *out_1_2 = z5;
+  *out_0_3 = z6;
+  *out_1_3 = z7;
+}
+
+static inline void _xa_nn_dot_product_1_row_4_vecs_unaligned
+    (xtfloatx2* out_0_0
+    ,xtfloatx2* out_1_0
+    ,xtfloat*   py
+    ,xtfloat*   pv0
+    ,WORD32     cols1
+    ,WORD32     vec_offset
+    )
+{
+  xtfloatx2 acc00;
+  xtfloatx2 acc20, acc21, acc22, acc23;
+  xtfloatx2 acc30, acc31, acc32, acc33;
+  xtfloatx2 v00;
+  xtfloatx2 v20;
+  xtfloatx2 y0;
+
+  xtfloat* pv1 = pv0 + vec_offset; //next vec 
+  xtfloat* pv2 = pv1 + vec_offset; //next vec
+  xtfloat* pv3 = pv2 + vec_offset; //next vec 
+  
+  xtfloatx2 z0 = *out_0_0;
+  xtfloatx2 z1 = *out_1_0;
+
+  int k;
+
+  acc00 = ZERO_SX2();
+  acc20 = acc21 = acc30 = acc31 = ZERO_SX2();
+  acc22 = acc23 = acc32 = acc33 = ZERO_SX2();
+
+  acc20 =  ADD_SX2(ADD_SX2(acc20, acc21), ADD_SX2(acc22, acc23));
+  acc30 =  ADD_SX2(ADD_SX2(acc30, acc31), ADD_SX2(acc32, acc33));
+
+  acc00 = ZERO_SX2();
+  acc20 = ZERO_SX2();
+  xtfloatx2 _xtfloatx2_temp_00, _xtfloatx2_temp_20;
+  for(k = 0; k < cols1; k++, pv0++, pv1++, pv2++, pv3++, py++)
+  {
+      v00 = XT_SEL32_LL_SX2(AE_MOVXTFLOATX2_FROMXTFLOAT(*(pv0)), AE_MOVXTFLOATX2_FROMXTFLOAT(*(pv1)));
+      v20 = XT_SEL32_LL_SX2(AE_MOVXTFLOATX2_FROMXTFLOAT(*(pv2)), AE_MOVXTFLOATX2_FROMXTFLOAT(*(pv3)));
+      y0 = AE_MOVXTFLOATX2_FROMXTFLOAT(*(py));
+      MUL_SX2X2(_xtfloatx2_temp_00, _xtfloatx2_temp_20, v00, v20, y0, y0);
+      ADD_SX2X2(acc00, acc20, acc00, acc20, _xtfloatx2_temp_00, _xtfloatx2_temp_20);
+  }
+  z0 = ADD_SX2(z0, acc00);
+  z1 = ADD_SX2(z1, acc20);
+
+  *out_0_0 = z0;
+  *out_1_0 = z1;
+}
+
+WORD32 xa_nn_matmul_f32xf32_f32(
+    FLOAT32 * __restrict__ p_out,          
+    const FLOAT32 * __restrict__ p_mat1,   
+    const FLOAT32 * __restrict__ p_vec1,   
+    const FLOAT32 * __restrict__ pt_bias,   
+    WORD32 rows,
+    WORD32 cols1,
+    WORD32 row_stride1,                    
+    WORD32 vec_count,                      
+    WORD32 vec_offset,
+    WORD32 out_offset,
+    WORD32 out_stride)                      
+{
+    /* NULL pointer checks */
+    XA_NNLIB_ARG_CHK_PTR(p_out, -1);
+    /* Pointer alignment checks */
+    XA_NNLIB_ARG_CHK_ALIGN(p_out, sizeof(FLOAT32), -1);
+    XA_NNLIB_ARG_CHK_ALIGN(p_mat1, sizeof(FLOAT32), -1);
+    XA_NNLIB_ARG_CHK_ALIGN(p_vec1, sizeof(FLOAT32), -1);
+    XA_NNLIB_ARG_CHK_ALIGN(pt_bias, sizeof(FLOAT32), -1);
+    /* Basic Parameter checks */
+    XA_NNLIB_ARG_CHK_COND((rows <= 0), -1);
+    XA_NNLIB_ARG_CHK_COND((cols1 <= 0), -1);
+    XA_NNLIB_ARG_CHK_COND((row_stride1 < cols1), -1);
+    XA_NNLIB_ARG_CHK_COND((vec_offset == 0), -1);
+    XA_NNLIB_ARG_CHK_COND((out_offset == 0), -1);
+    XA_NNLIB_ARG_CHK_COND((out_stride == 0), -1);
+  
+    xtfloat *p_bias = (xtfloat*)pt_bias;
+    /* Iterators used in for loops */
+    int m_itr, c_itr, vec_itr;
+    /* Assign initial value so this value will be used in trailing loop */
+    m_itr = 0;
+    vec_itr = 0;
+
+    {
+      for (vec_itr = 0; vec_itr < (vec_count & ~(4-1)); vec_itr += 4)
+      {
+        xtfloat *p_out_0 = (xtfloat*)(p_out + (vec_itr + 0)*out_offset);
+        xtfloat *p_out_1 = (xtfloat*)(p_out + (vec_itr + 1)*out_offset);
+        xtfloat *p_out_2 = (xtfloat*)(p_out + (vec_itr + 2)*out_offset);
+        xtfloat *p_out_3 = (xtfloat*)(p_out + (vec_itr + 3)*out_offset);
+        int ii;
+        for(m_itr = 0; m_itr < (rows & ~(16 - 1)); m_itr += 16)
+        {
+          for(ii = 0; ii < 4; ii++)
+          {
+            xtfloat *p_out_0_ii = p_out_0 + (m_itr + ii) * out_stride;
+            xtfloat *p_out_1_ii = p_out_1 + (m_itr + ii) * out_stride;
+            xtfloat *p_out_2_ii = p_out_2 + (m_itr + ii) * out_stride;
+            xtfloat *p_out_3_ii = p_out_3 + (m_itr + ii) * out_stride;
+            /* Init out registers with bias */
+            xtfloatx2 z0, z1, z2, z3;
+            xtfloatx2 z4, z5, z6, z7;
+            z0 = z1 = z2 = z3 = ZERO_SX2();
+            z4 = z5 = z6 = z7 = ZERO_SX2();
+            if(p_bias != NULL)
+            {
+              z6 = z4 = z2 = z0 = XT_SEL32_LL_SX2(AE_MOVXTFLOATX2_FROMXTFLOAT(p_bias[m_itr+ii+0]), AE_MOVXTFLOATX2_FROMXTFLOAT(p_bias[m_itr+ii+4]));
+              z7 = z5 = z3 = z1 = XT_SEL32_LL_SX2(AE_MOVXTFLOATX2_FROMXTFLOAT(p_bias[m_itr+ii+8]), AE_MOVXTFLOATX2_FROMXTFLOAT(p_bias[m_itr+ii+12]));
+            }
+            
+            xtfloat *p_mat = (xtfloat *)(p_mat1+((m_itr+ii)*row_stride1));
+            xtfloat *p_vec = (xtfloat *)(p_vec1+(vec_itr*vec_offset));
+
+            _xa_nn_dot_product_4_rows_4_vecs_offset_aligned
+              (&z0
+              ,&z1
+              ,&z2
+              ,&z3
+              ,&z4
+              ,&z5
+              ,&z6
+              ,&z7
+              ,(xtfloat *)p_mat
+              ,(xtfloat *)p_vec
+              ,cols1
+              ,row_stride1
+              ,vec_offset
+              );
+            
+            XT_SSXP(AE_MOVXTFLOAT_FROMXTFLOATX2(XT_SEL32_HH_SX2(z0,z0)), p_out_0_ii, 4*out_stride*sizeof(xtfloat));
+            XT_SSXP(AE_MOVXTFLOAT_FROMXTFLOATX2(z0), p_out_0_ii, 4*out_stride*sizeof(xtfloat));
+            XT_SSXP(AE_MOVXTFLOAT_FROMXTFLOATX2(XT_SEL32_HH_SX2(z1,z1)), p_out_0_ii, 4*out_stride*sizeof(xtfloat));
+            XT_SSXP(AE_MOVXTFLOAT_FROMXTFLOATX2(z1), p_out_0_ii, 4*out_stride*sizeof(xtfloat));
+            XT_SSXP(AE_MOVXTFLOAT_FROMXTFLOATX2(XT_SEL32_HH_SX2(z2,z2)), p_out_1_ii, 4*out_stride*sizeof(xtfloat));
+            XT_SSXP(AE_MOVXTFLOAT_FROMXTFLOATX2(z2), p_out_1_ii, 4*out_stride*sizeof(xtfloat));
+            XT_SSXP(AE_MOVXTFLOAT_FROMXTFLOATX2(XT_SEL32_HH_SX2(z3,z3)), p_out_1_ii, 4*out_stride*sizeof(xtfloat));
+            XT_SSXP(AE_MOVXTFLOAT_FROMXTFLOATX2(z3), p_out_1_ii, 4*out_stride*sizeof(xtfloat));
+            XT_SSXP(AE_MOVXTFLOAT_FROMXTFLOATX2(XT_SEL32_HH_SX2(z4,z4)), p_out_2_ii, 4*out_stride*sizeof(xtfloat));
+            XT_SSXP(AE_MOVXTFLOAT_FROMXTFLOATX2(z4), p_out_2_ii, 4*out_stride*sizeof(xtfloat));
+            XT_SSXP(AE_MOVXTFLOAT_FROMXTFLOATX2(XT_SEL32_HH_SX2(z5,z5)), p_out_2_ii, 4*out_stride*sizeof(xtfloat));
+            XT_SSXP(AE_MOVXTFLOAT_FROMXTFLOATX2(z5), p_out_2_ii, 4*out_stride*sizeof(xtfloat));
+            XT_SSXP(AE_MOVXTFLOAT_FROMXTFLOATX2(XT_SEL32_HH_SX2(z6,z6)), p_out_3_ii, 4*out_stride*sizeof(xtfloat));
+            XT_SSXP(AE_MOVXTFLOAT_FROMXTFLOATX2(z6), p_out_3_ii, 4*out_stride*sizeof(xtfloat));
+            XT_SSXP(AE_MOVXTFLOAT_FROMXTFLOATX2(XT_SEL32_HH_SX2(z7,z7)), p_out_3_ii, 4*out_stride*sizeof(xtfloat));
+            XT_SSXP(AE_MOVXTFLOAT_FROMXTFLOATX2(z7), p_out_3_ii, 4*out_stride*sizeof(xtfloat));
+          }
+        }
+        p_out_0 = p_out_0 + (rows & (~15)) * out_stride;
+        p_out_1 = p_out_1 + (rows & (~15)) * out_stride;
+        p_out_2 = p_out_2 + (rows & (~15)) * out_stride;
+        p_out_3 = p_out_3 + (rows & (~15)) * out_stride;
+        
+        //Remaining (rows % 16) rows
+        for(m_itr = (rows & ~(15)); m_itr < rows; m_itr++)
+        {
+          /* Init out registers with bias */
+          xtfloatx2 z0, z1;
+          z0 = z1 = ZERO_SX2();
+          if(p_bias != NULL)
+          {
+            z0 = z1 = AE_MOVXTFLOATX2_FROMXTFLOAT(p_bias[m_itr]);
+          }
+          
+          xtfloat *p_mat = (xtfloat *)(p_mat1+(m_itr*row_stride1));
+          xtfloat *p_vec = (xtfloat *)(p_vec1+(vec_itr*vec_offset));
+
+          _xa_nn_dot_product_1_row_4_vecs_unaligned
+            (&z0
+            ,&z1
+            ,(xtfloat *)p_mat
+            ,(xtfloat *)p_vec
+            ,cols1
+            ,vec_offset
+            );
+         
+          AE_SSXP(AE_MOVXTFLOAT_FROMXTFLOATX2(XT_SEL32_HH_SX2(z0,z0)), p_out_0, out_stride*sizeof(xtfloat));
+          AE_SSXP(AE_MOVXTFLOAT_FROMXTFLOATX2(z0), p_out_1, out_stride*sizeof(xtfloat));
+          AE_SSXP(AE_MOVXTFLOAT_FROMXTFLOATX2(XT_SEL32_HH_SX2(z1,z1)), p_out_2, out_stride*sizeof(xtfloat));
+          AE_SSXP(AE_MOVXTFLOAT_FROMXTFLOATX2(z1), p_out_3, out_stride*sizeof(xtfloat));
+        }
+      }
+      /* Tail loop for vec unroll */
+      for(vec_itr = (vec_count & ~(3)); vec_itr < vec_count; vec_itr++)
+      {
+        int ii;
+        xtfloat *p_out_0 = (xtfloat *)(p_out + (vec_itr*out_offset));
+        for(m_itr = 0; m_itr < (rows & ~(16 - 1)); m_itr += 16)
+        {
+          for(ii = 0; ii < 4; ii++)
+          {
+              xtfloat *p_out_0_ii = p_out_0 + (m_itr + ii) * out_stride;
+              /* Init out registers with bias */
+              xtfloatx2 z0, z1;
+              z0 = z1 = ZERO_SX2();
+              if(p_bias != NULL)
+              {
+                z0 = XT_SEL32_LL_SX2(AE_MOVXTFLOATX2_FROMXTFLOAT(p_bias[m_itr+ii+0]), AE_MOVXTFLOATX2_FROMXTFLOAT(p_bias[m_itr+ii+4]));
+                z1 = XT_SEL32_LL_SX2(AE_MOVXTFLOATX2_FROMXTFLOAT(p_bias[m_itr+ii+8]), AE_MOVXTFLOATX2_FROMXTFLOAT(p_bias[m_itr+ii+12]));
+              }
+              
+              xtfloat *p_mat = (xtfloat *)(p_mat1+((m_itr+ii)*row_stride1));
+              xtfloat *p_vec = (xtfloat *)(p_vec1+(vec_itr*vec_offset));
+
+              _xa_nn_dot_product_4_rows_1_vecs_offset_aligned
+                (&z0
+                ,&z1
+                ,(xtfloat *)p_mat
+                ,(xtfloat *)p_vec
+                ,cols1
+                ,row_stride1
+                );
+              
+              XT_SSXP(AE_MOVXTFLOAT_FROMXTFLOATX2(XT_SEL32_HH_SX2(z0,z0)), p_out_0_ii, 4*out_stride*sizeof(xtfloat));
+              XT_SSXP(AE_MOVXTFLOAT_FROMXTFLOATX2(z0), p_out_0_ii, 4*out_stride*sizeof(xtfloat));
+              XT_SSXP(AE_MOVXTFLOAT_FROMXTFLOATX2(XT_SEL32_HH_SX2(z1,z1)), p_out_0_ii, 4*out_stride*sizeof(xtfloat));
+              XT_SSXP(AE_MOVXTFLOAT_FROMXTFLOATX2(z1), p_out_0_ii, 4*out_stride*sizeof(xtfloat));
+            }
+        }
+
+        p_out_0 = p_out_0 + (rows & (~15)) * out_stride;
+        xtfloat bias = ZERO_S();
+        xtfloat *pbias = (xtfloat *) p_bias + m_itr;
+        for(m_itr = (rows & ~(15)); m_itr < rows; m_itr++)
+        {
+          xtfloatx2 acc_row0_vec0;
+          xtfloatx2 _xtfloatx2_temp;
+          xtfloatx2 vec_batch_0_0;
+          xtfloatx2 mat1_0_0;
+          
+          xtfloat *p_vec_batch_0  = (xtfloat *)(p_vec1 + (vec_itr + 0)*vec_offset);
+          xtfloat *p_mat1_0 = (xtfloat *) &p_mat1[(m_itr+0)*row_stride1];
+          
+          acc_row0_vec0 = ZERO_SX2();
+          
+          /* Remainder loop for cols1 */
+          for(c_itr = 0; c_itr < cols1; c_itr++, 
+              p_vec_batch_0++, p_mat1_0++)
+          {
+              vec_batch_0_0 = AE_MOVXTFLOATX2_FROMXTFLOAT(*((xtfloat *)p_vec_batch_0));
+              mat1_0_0 = AE_MOVXTFLOATX2_FROMXTFLOAT(*((xtfloat *)p_mat1_0));
+              _xtfloatx2_temp = MUL_SX2(vec_batch_0_0, mat1_0_0);
+              acc_row0_vec0 = ADD_SX2(acc_row0_vec0, _xtfloatx2_temp);
+          }
+          if(p_bias!=NULL)
+          {
+            XT_LSIP(bias, pbias, 4);
+            acc_row0_vec0 = AE_MOVXTFLOATX2_FROMXTFLOAT(ADD_S(AE_MOVXTFLOAT_FROMXTFLOATX2(acc_row0_vec0), bias));
+          }
+         
+          XT_SSXP(AE_MOVXTFLOAT_FROMXTFLOATX2(acc_row0_vec0), p_out_0, out_stride*sizeof(xtfloat));
+        }
+      }
+    }
+
+    return 0;
+}
+
+#else /* NO_AGGR_FLOAT_OPT */
+
 /* Using the 4 row 1 vec function defined in xa_nn_matXvec_f32.c for xa_nn_matXvec_f32() kernel */
 extern void _xa_nn_dot_product_4_rows_1_vecs_offset_aligned
     (xtfloatx2* out_0_0
@@ -1031,4 +1428,5 @@ WORD32 xa_nn_matmul_f32xf32_f32(
 
     return 0;
 }
+#endif /* NO_AGGR_FLOAT_OPT */
 #endif
